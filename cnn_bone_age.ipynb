{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting age of a child by looking at a hand Xray\n",
    "The goal of this project is to identify the age of a child from an X-ray of their hand. This data was part of a competition http://rsnachallenges.cloudapp.net/competitions/4. We will build a model inspired by the one used by the winners of the competition. You will also borrow ideas from this fast.ai notebook. https://github.com/fastai/fastai/blob/master/courses/dl2/cifar10-darknet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data you can install the kaggle api using: <br/>\n",
    "`pip install kaggle` <br/>\n",
    "To get the dataset you can use this command line. (If you don't provide a path you will find your data in /home/user/.kaggle/datasets/kmader/rsna-bone-age/ )<br/>\n",
    "`kaggle datasets download -d kmader/rsna-bone-age -p PATH` <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/data2/yinterian/rsna-bone-age/model007.pth'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/model.pth'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/rsna-bone-age.zip'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/boneage-test-dataset.csv'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/model046.pth'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/boneage-training-dataset'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/mode_tmp.pth'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/boneage-test-dataset.zip'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/boneage-550'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/boneage-training-dataset.zip'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/model003.pth'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/mode1045.pth'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/model001.pth'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/boneage-training-dataset.csv'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/boneage-test-dataset'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/mode991.pth'),\n",
       " PosixPath('/data2/yinterian/rsna-bone-age/model0005.pth')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path(\"/data2/yinterian/rsna-bone-age/\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,boneage,male\r\n",
      "1377,180,False\r\n",
      "1378,12,False\r\n",
      "1379,94,False\r\n",
      "1380,120,True\r\n",
      "1381,82,False\r\n",
      "1382,138,True\r\n",
      "1383,150,True\r\n",
      "1384,156,True\r\n",
      "1385,36,True\r\n"
     ]
    }
   ],
   "source": [
    "! head /data2/yinterian/rsna-bone-age/boneage-training-dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = PATH/\"boneage-training-dataset/9977.png\"\n",
    "im = cv2.imread(str(path))  #.astype(np.float32)/255\n",
    "plt.imshow(im, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread(str(path))  #.astype(np.float32)/255\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the typical size of these images\n",
    "path = PATH/\"boneage-training-dataset\"\n",
    "files = list(path.iterdir())[:200]\n",
    "dims = [cv2.imread(str(p)).shape for p in files]\n",
    "dims[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = [x[0]/x[1] for x in dims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a presentation with EDA on this data\n",
    "https://alxndrkalinin.github.io/pdf/2017-12_CFT_BoneAge.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "The winners of the competition used real-time image augmentation consisting of horizontal/vertical translation, zoom, and rotation of 20 percent/degrees as well as horizontal flip. They use 500x500 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from fast.ai\n",
    "import math\n",
    "def crop(im, r, c, target_r, target_c): return im[r:r+target_r, c:c+target_c]\n",
    "\n",
    "def center_crop(im, min_sz=None):\n",
    "    \"\"\" Returns a center crop of an image\"\"\"\n",
    "    r,c,*_ = im.shape\n",
    "    if min_sz is None: min_sz = min(r,c)\n",
    "    start_r = math.ceil((r-min_sz)/2)\n",
    "    start_c = math.ceil((c-min_sz)/2)\n",
    "    return crop(im, start_r, start_c, min_sz, min_sz)\n",
    "\n",
    "def random_crop(x, target_r, target_c):\n",
    "    r,c,*_ = x.shape\n",
    "    rand_r = random.uniform(0, 1)\n",
    "    rand_c = random.uniform(0, 1)\n",
    "    start_r = np.floor(rand_r*(r - target_r)).astype(int)\n",
    "    start_c = np.floor(rand_c*(c - target_c)).astype(int)\n",
    "    return crop(x, start_r, start_c, target_r, target_c)\n",
    "\n",
    "def rotate_cv(im, deg, mode=cv2.BORDER_REFLECT, interpolation=cv2.INTER_AREA):\n",
    "    \"\"\" Rotates an image by deg degrees\"\"\"\n",
    "    r,c,*_ = im.shape\n",
    "    M = cv2.getRotationMatrix2D((c/2,r/2),deg,1)\n",
    "    return cv2.warpAffine(im,M,(c,r), borderMode=mode, flags=cv2.WARP_FILL_OUTLIERS+interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Center crop, resize, horizontal and vertical translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize desforms the image a bit\n",
    "# note that by resizing to a larger number and random cropping we are doing horizontal and vertical translations\n",
    "# we should try just center cropping the image instead of resizing\n",
    "path = PATH/\"boneage-training-dataset/9977.png\"\n",
    "im = cv2.imread(str(path))\n",
    "im = center_crop(im)\n",
    "im = cv2.resize(im, (550, 550))\n",
    "im = random_crop(im, 500, 500)\n",
    "plt.imshow(im, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Rotation (-10, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdeg = (np.random.random()-.50)*20\n",
    "print(rdeg)\n",
    "im_rot = rotate_cv(im, rdeg)\n",
    "plt.imshow(im_rot, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_f = np.fliplr(im)\n",
    "plt.imshow(im_f, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH/\"boneage-training-dataset.csv\")\n",
    "train = df.sample(frac=0.8, random_state=3).copy()\n",
    "valid = df.drop(train.index).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Saving a resized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Do this to save time at training.\n",
    "PATH_550 = PATH/\"boneage-550\"\n",
    "#PATH_550.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_a_crop(path, sz=550):\n",
    "    im = cv2.imread(str(path))\n",
    "    r,c,_ = im.shape\n",
    "    pad = abs(r-c)//4\n",
    "    if r > c :\n",
    "        im2 = cv2.copyMakeBorder(im, 0, 0, pad, pad, cv2.BORDER_REFLECT)\n",
    "    else:\n",
    "        im2 = cv2.copyMakeBorder(im, pad, pad, 0, 0, cv2.BORDER_REFLECT)\n",
    "    return cv2.resize(center_crop(im2), (sz, sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "def resize_all_images(sz):\n",
    "    for f in listdir(PATH/\"boneage-training-dataset/\"):\n",
    "        old_path = join(PATH/\"boneage-training-dataset/\", f)\n",
    "        new_path = join(PATH/\"boneage-550/\", f)\n",
    "        img2 = get_a_crop(old_path,sz)\n",
    "        cv2.imwrite(new_path, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path = PATH/\"boneage-training-dataset/10007.png\"\n",
    "im2 = get_a_crop(path)\n",
    "print(im2.shape)\n",
    "plt.imshow(im2, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoneAgeDataset(Dataset):\n",
    "    def __init__(self, df, transforms=True, sz=400):\n",
    "        self.path_to_images = PATH/\"boneage-550/\"\n",
    "        self.transforms = transforms\n",
    "        self.df = df\n",
    "        self.sz = sz\n",
    "        self.sz2 = int(sz*1.05)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = str(self.path_to_images) + \"/\" + str(row[\"id\"]) + \".png\"\n",
    "        y = row[\"boneage\"]\n",
    "        x = cv2.imread(str(path)).astype(np.float32)/255\n",
    "        x = center_crop(x)\n",
    "        if self.transforms:\n",
    "            x = cv2.resize(x, (self.sz2, self.sz2))\n",
    "            x = random_crop(x, self.sz, self.sz)\n",
    "            rdeg = (np.random.random()-.50)*20\n",
    "            x = rotate_cv(x, rdeg)\n",
    "            if np.random.random() > 0.8: x = np.fliplr(x).copy() \n",
    "        else:\n",
    "            x = cv2.resize(x, (self.sz, self.sz))\n",
    "        x = x[:,:,0]\n",
    "        return x[None], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BoneAgeDataset(train)\n",
    "valid_ds = BoneAgeDataset(valid, transforms=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this multiple times to get different images\n",
    "x, y = train_ds[10]\n",
    "#plt.imshow(x[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "This model is adapted from fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From fast.ai\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, x): return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From fast.ai\n",
    "def conv_layer(ni, nf, ks=3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ni, nf, kernel_size=ks, bias=False, stride=stride, padding=ks//2),\n",
    "        nn.BatchNorm2d(nf, momentum=0.01),\n",
    "        nn.LeakyReLU(negative_slope=0.1, inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResLayer(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.conv1=conv_layer(ni, ni//2, ks=1)\n",
    "        self.conv2=conv_layer(ni//2, ni, ks=3)\n",
    "        \n",
    "    def forward(self, x): return x.add(self.conv2(self.conv1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "    def make_group_layer(self, ch_in, num_blocks, stride=1):\n",
    "        return [conv_layer(ch_in, ch_in*2,stride=stride)\n",
    "               ] + [(ResLayer(ch_in*2)) for i in range(num_blocks)]\n",
    "\n",
    "    def __init__(self, num_blocks, nf=32):\n",
    "        super().__init__()\n",
    "        layers = [conv_layer(1, nf, ks=3, stride=1)]\n",
    "        for i,nb in enumerate(num_blocks):\n",
    "            layers += self.make_group_layer(nf, nb, stride=2)\n",
    "            nf *= 2\n",
    "        layers += [nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(nf, 1)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x): return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Darknet([1, 2, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = m.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_dl))\n",
    "x = Variable(x).cuda().float()\n",
    "y = Variable(y).cuda().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1e-5*100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal learning rate range\n",
    "From this paper https://arxiv.org/pdf/1506.01186.pdf.\n",
    "This an implementation of the \"LR range test\". Run your model for several epochs while letting the learning rate increase linearly between low and high LR values. Next, plot the loss versus learning rate. Note the learning rate value when the loss starts to decrease and when the loss slows, becomes ragged, or increases. In the example below the range seem to be from `1e-5` to `0.012`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr = 0.01, wd = 0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))\n",
    "\n",
    "def LR_range_finder(model, train_dl, lr_low=1e-5, lr_high=1, epochs=2):\n",
    "    losses = []\n",
    "    p = PATH/\"mode_tmp.pth\"\n",
    "    save_model(model, p)\n",
    "    iterations = epochs * len(train_dl)\n",
    "    delta = (lr_high - lr_low)/iterations\n",
    "    lrs = [lr_low + i*delta for i in range(iterations)]\n",
    "    model.train()\n",
    "    ind = 0\n",
    "    for i in range(epochs):\n",
    "        for j, (x, y) in enumerate(train_dl):\n",
    "            optim = get_optimizer(model, lr=lrs[ind])\n",
    "            x = Variable(x).cuda().float()\n",
    "            y = Variable(y).cuda().float()\n",
    "            out = model(x)\n",
    "            loss = F.l1_loss(out, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            losses.append(loss.data[0])\n",
    "            ind +=1\n",
    "            \n",
    "    load_model(model, p)\n",
    "    return lrs, losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-508993f3b147>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDarknet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR_range_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-385de5b0396a>\u001b[0m in \u001b[0;36mLR_range_finder\u001b[0;34m(model, train_dl, lr_low, lr_high, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-12c538b05133>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_to_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boneage\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcenter_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Darknet([1, 2, 4, 6, 3]).cuda()\n",
    "lrs, losses = LR_range_finder(model, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first plot the whole graph\n",
    "plt.plot(lrs[:80], losses[:80])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning with Triangular learning rate policy.\n",
    "\n",
    "Before training with this policy you have to estimate the range of learning rates using previous function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.1111111111111111, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.5555555555555556, 0.6666666666666666, 0.7777777777777777, 0.8888888888888888, 1.0, 1.0, 0.8888888888888888, 0.7777777777777778, 0.6666666666666667, 0.5555555555555556, 0.4444444444444444, 0.33333333333333337, 0.22222222222222232, 0.11111111111111116, 0.0]\n"
     ]
    }
   ],
   "source": [
    "def get_triangular_lr(lr_low, lr_high, stepesize):\n",
    "    delta = (lr_high - lr_low)/(stepesize -1)\n",
    "    lrs1 = [lr_low + i*delta for i in range(stepesize)]\n",
    "    lrs2 = [lr_high - i*delta for i in range(stepesize)]\n",
    "    return lrs1+lrs2\n",
    "lrs = get_triangular_lr(0, 1, 10)\n",
    "print(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triangular_lr2(lr_low, lr_high, stepesize):\n",
    "    iterations = 2*stepesize\n",
    "    iter1 = int(0.35*iterations)\n",
    "    iter2 = int(0.85*iter1)\n",
    "    iter3 = iterations - iter1 - iter2\n",
    "    delta1 = (lr_high - lr_low)/iter1\n",
    "    delta2 = (lr_high - lr_low)/(iter1 -1)\n",
    "    lrs1 = [lr_low + i*delta1 for i in range(iter1)]\n",
    "    lrs2 = [lr_high - i*(delta1) for i in range(0, iter2)]\n",
    "    delta2 = (lrs2[-1] - lr_low)/(iter3)\n",
    "    lrs3 = [lrs2[-1] - i*(delta2) for i in range(1, iter3+1)]\n",
    "    return lrs1+lrs2+lrs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl4VfW97/H3NwkJc4AkjAkQIAFRwCHiCCgIBNtKe2pbrD215zh0EIejVPH23vOc23MHaXHGto7n2lZFj6cDVRksIEOtQFBGIQNhCoMEgmEykOF3/9grMcRAdnZ2svbO/ryeh4e91/6tle9aT/b+Zk2fbc45RERE4vwuQEREIoMagoiIAGoIIiLiUUMQERFADUFERDxqCCIiAqghiIiIRw1BREQANQQREfEk+F1Ac6SmprrBgwf7XYaISFRZv379YedcWlPjoqohDB48mLy8PL/LEBGJKma2O5hxOmQkIiKAGoKIiHjUEEREBFBDEBERjxqCiIgAQTYEM8s1s3wzKzKz2Y28nmRmb3ivrzGzwd70FDNbbmYnzGxevfGdzewdM9tuZlvN7NFwrZCIiISmyYZgZvHAs8A0YCRwi5mNbDDsduCoc24Y8AQwx5teAfwPYFYji57rnBsBXAJcY2bTQlsFEREJh2D2EMYCRc65YufcGWA+ML3BmOnAK97jt4BJZmbOuZPOudUEGkMd59wp59xy7/EZ4CMgvQXrIe3AkROneXvTfvS1riL+CKYhDAD21nte4k1rdIxzrgooB1KCKcDMegBfA5ae4/W7zCzPzPJKS0uDWaREqdl/2MzM1z5mef4hv0sRiUnBNARrZFrDP+GCGfPlBZslAK8DTzvnihsb45x73jmX45zLSUtr8s5riVJ5u8p475NPiY8z5izMp7pGewkibS2YhlACZNR7ng7sP9cY70M+GSgLYtnPA4XOuSeDGCvtlHOORxduJ61bEv/3H0aR/+lx/vjxPr/LEok5wTSEdUCWmWWaWSIwA1jQYMwC4Dbv8c3AMtfEgWAz+18EGsf9zStZ2pul2w6Rt/so99+Qxc2XpjM6PZkn3iugorLa79JEYkqTDcE7JzATWAxsA950zm01s5+b2U3esJeAFDMrAh4A6i5NNbNdwOPAD8ysxMxGmlk68DMCVy19ZGYbzOyOcK6YRIfqGsecRdsZktqFb+dkEBdnzM4dwb7PPuf3HwaVxyUiYRJU2qlz7l3g3QbT/rXe4wrgW+eYd/A5FtvYeQeJMf/1UQmFh07wq1svpUN84O+Tq4elMi4rlXnLi/hWTgbJnTr4XKVIbNCdyuKbispqnnivgDEZPZh2Ud+zXns4dwSfnarkuRU7fKpOJPaoIYhvXvlgFwfKK5idOwKzs3cYLxqQzPSL+/Py33by6bGKcyxBRMJJDUF8UX6qkl+9v4Prhqdx1dDGb1l5cPJwqmscT/61sI2rE4lNagjii1+v2MGxikoemjrinGMGpnTm1isG8WbeXooOnWjD6kRikxqCtLkD5Z/zH3/bydcvHsDI/t3PO3bmxGF0TIhj7uL8NqpOJHapIUibe/K9QpyDByZnNzk2tWsSd40fyqKtB/loz9E2qE4kdqkhSJsqOnSc/1y/l+9dOYiMXp2DmueOcZmkdk3k0YXbFXwn0orUEKRN/WJRPp0TE5g5cVjQ83RJSuDeSVms3VnG+/kKOBRpLWoI0mbW7y5jySef8sPxQ+jVJbFZ8864fCCDUjozZ9F2Bd+JtBI1BGkTtQF2qV2TuH1cZrPnT0yIY9aU4Ww/eJw/KfhOpFWoIUibWLrtEOt2BQLsOicGlZjyJV8Z1Y9RA5J5XMF3Iq1CDUFaXXWN4xeLt5OZ2oXvXJ7R9AznEBdnzJ6m4DuR1qKGIK3uDx+VUPDpCWZNGV4XYBeqa+oF3x2rqAxThSICagjSyioqq3n8vQLGpCdz46i+Tc8QBAXfibQONQRpVb/9eyDA7uFpXw6wC9VFA5K5aUx/Xlqt4DuRcFJDkFZT/nklzy7fwYTsNK4emhrWZc+aEgi+e2qpgu9EwkUNQVrNb7wAu4dzzx1gF6qBKZ357tiBvLFuLztKFXwnEg5qCNIqDpZX8PLqnUwf07/JALtQ3TMpS8F3ImGkhiCt4sm/FlDjHA9OGd5qPyO1axJ3jh/Cwi0KvhMJBzUECbuiQ8d5M695AXahumPcEFK7JjJHwXciLaaGIGH3y8VegN31wQfYhaqrF3y3ZmcZ7xco+E6kJdQQJKzW7z7K4q2fctf4IaR0TWqTnznj8oEM7NWZOQsVfCfSEmoIEjbOOebUBthd2/wAu1AlJsQxa2og+O7PGxR8JxIqNQQJm2XbD7F2Vxn33ZBFl6TQAuxC9dVR/bhoQHceW1LA6SoF34mEIqiGYGa5ZpZvZkVmNruR15PM7A3v9TVmNtibnmJmy83shJnNazDPZWa22ZvnaQvXbazii+oaxy8W5TM4pTMzWhBgF6q4OGN27gVe8N2eNv/5Iu1Bkw3BzOKBZ4FpwEjgFjMb2WDY7cBR59ww4Algjje9AvgfwKxGFv1r4C4gy/uXG8oKSGT448f7yP/0OD+dOqLFAXahujYrlWuHpTJvWaGC70RCEMw7dyxQ5Jwrds6dAeYD0xuMmQ684j1+C5hkZuacO+mcW02gMdQxs35Ad+fc313gWsHfAl9vyYqIfyoqq3l8ST6jwxhgF6qHc0dw9FQlz68o9rUOkWgUTEMYAOyt97zEm9boGOdcFVAOpDSxzJImlgmAmd1lZnlmlldaqssKI9Hv/r6b/eUVzM4NX4BdqEalJ/O1Mf15cXUxhxR8J9IswTSExt7hDa/tC2ZMSOOdc88753KcczlpaWnnWaT4ofzzSuYtL2J8dhpXDwtvgF2oZk3Jpqra8aSC70SaJZiGUALUP0uYDuw/1xgzSwCSgbImlpnexDIlCjy3Ygfln1fycG7rRVQ016CULtx6RSD4rljBdyJBC6YhrAOyzCzTzBKBGcCCBmMWALd5j28Glrnz5Ag45w4Ax83sSu/qou8Df2529eKrg+UVvPy3nUy/uD8X9k/2u5yzzJyYRVJCHHOXKPhOJFhNNgTvnMBMYDGwDXjTObfVzH5uZjd5w14CUsysCHgAqLs01cx2AY8DPzCzknpXKP0YeBEoAnYAC8OzStJWnlpaQHWN48HJkbN3UCutWxJ3jhvCu5sP8rGC70SCYtEUCJaTk+Py8vL8LkOAokMnmPLECr5/1WD+7aYL/S6nUSdOVzHhF8vJ6tOV1++80vcT3iJ+MbP1zrmcpsbpTmUJyVwvwO6eia0fYBeq2uC7D4vLWKHgO5EmqSFIs3205yiLth5s0wC7UN0yNhB89+jC7dQo+E7kvNQQpFmcczz6btsH2IUqMSGOB6dkB4LvNir4TuR81BCkWZbnewF2k4a1eYBdqL42uj8X9u/O3MUKvhM5HzUECdpZAXZjB/pdTtDi4ozZ00Yo+E6kCWoIErQ/fbyP7QePM2vqcN8C7EI1LitNwXciTYiud7X4pqKymsffK2DUgGRuvKif3+WEpDb47oWVCr4TaYwaggTl9x/uZt9nnzN72gji4qLzev5R6cl8dXQ/Xly1U8F3Io1QQ5AmHasIBNiNy0rlmggJsAvVrCnDqayu4SkF34l8iRqCNOm5FTv47FQlD+eO8LuUFhuc2oXvXjGQ+Qq+E/kSNQQ5r0+PVfDS6kCA3UUDIivALlT3eMF3jy0p8LsUkYiihiDn9eRfCyM2wC5Uad2SuGPcEN7ZfIANez/zuxyRiKGGIOe0o/QEb+bt5dYrBjEwpbPf5YTVneMySemSyKMLtxFNAY8irUkNQc5p7uJ8OibEMTOCA+xC1a1jB+6ZOEzBdyL1qCFIoz7ac5SFWw5y1/ihpEZ4gF2ovnvFIDJ6dWLOonwF34mghiCNcM7x6MLtpHZN5I5xkR9gF6rEhDhmTRnOtgPHWLBR3+AqooYgX/J+filrd5Zx76SsqAmwC9XXRvdnZL/uzF2Sr+A7iXlqCHKW6hrHnEXbGZTSmRmXR0+AXahqg+9Kjn7Oqwq+kxinhiBn+fMGL8BuynASE2Lj1yNwB3YK85YXcVzBdxLDYuMdL0GpqKzmsSWBALuvjIrOALtQmBkP546g7OQZBd9JTFNDkDq1AXYP50ZvgF2oRqf34Cuj+/HCqp0cOq7gO4lNaggCnB1gd21WdAfYhao2+O5pBd9JjFJDEACeX1HcbgLsQpWZ2oVbxg7k9bV72Xn4pN/liLQ5NQTh0LEKXlxdzE1j2k+AXajumTSMpIQ45i7J97sUkTYXVEMws1wzyzezIjOb3cjrSWb2hvf6GjMbXO+1R7zp+WY2td70fzGzrWa2xcxeN7OO4Vghab4nlxZSVe14cEq236X4rne3jtxxbSbvbDrARgXfSYxpsiGYWTzwLDANGAncYmYjGwy7HTjqnBsGPAHM8eYdCcwALgRygV+ZWbyZDQDuBXKccxcB8d44aWM7Sk/wxrq93HrFQAaldPG7nIhw5/gh9OqSyKMLtyv4TmJKMHsIY4Ei51yxc+4MMB+Y3mDMdOAV7/FbwCQzM2/6fOfcaefcTqDIWx5AAtDJzBKAzoCyA3xQG2B3z6Qsv0uJGLXBd38vPsLKwsN+lyPSZoJpCAOAvfWel3jTGh3jnKsCyoGUc83rnNsHzAX2AAeAcufcksZ+uJndZWZ5ZpZXWqpUynD62Auwu3P8kHYbYBeq714xMBB8t3C7gu8kZgTTEBq7IL3hO+RcYxqdbmY9Cew9ZAL9gS5m9r3Gfrhz7nnnXI5zLictLS2IciUYZwfYDfG7nIiTlBDPg5OH88mBY/xlk3ZeJTYE0xBKgIx6z9P58uGdujHeIaBkoOw8894A7HTOlTrnKoE/AFeHsgISmvcLSlmzs4x7JmbRtZ0H2IXqpjH9uaBfd365WMF3EhuCaQjrgCwzyzSzRAInfxc0GLMAuM17fDOwzAXOxi0AZnhXIWUCWcBaAoeKrjSzzt65hknAtpavjgSjpsYxZ+F2BvbqzC1j23+AXajqB9+9tkbBd9L+NdkQvHMCM4HFBD6033TObTWzn5vZTd6wl4AUMysCHgBme/NuBd4EPgEWAXc756qdc2sInHz+CNjs1fF8WNdMzunPG70Au6mxE2AXqvFZqVw9NIVnlin4Tto/i6bL6nJyclxeXp7fZUS101XVTJy7gp5dOrDg7mtjLrMoFBv3fsb0Z//GvZOyeGCy7tWQ6GNm651zOU2N05+HMeb3H+6J2QC7UI3J6MFXRvXjxVXFCr6Tdk0NIYYcq6hk3rJCrh2WyrgsXbHVHLOmDud0VQ3PLC3yuxSRVqOGEEOeX1HM0RgPsAtVIPgug9fX7mGXgu+knVJDiBGHjlXw0uqdfG1Mf0alx3aAXajunZRFh3gF30n7pYYQI55aWkhldQ2zFGAXst7dOnLHuEze3nSATSUKvpP2Rw0hBhSXnmD+ur18VwF2LXbX+CH07NxBwXfSLqkhxIC5S/JJSojjnokKsGupQPBdFh/sOMIqBd9JO6OG0M5t2PsZ724+yJ3jhpDWTQF24XDrlQNJ79mJOYsUfCftixpCOxYIsNtGSpdE7hyvALtwSUqI58Ep2Wzdr+A7aV/UENqxFQWlfFhcxr2TFGAXbtPHDOCCft2ZuySfM1U1fpcjEhZqCO1UTU0g3loBdq0jLs54OHc4e8s+57U1u/0uRyQs1BDaqdoAuwenZCvArpVMyE7jqiGB4LsTp6v8LkekxfRJ0Q6drqrmsSUFXNi/O18b3d/vctots0A89pGTZ3hhZbHf5Yi0mBpCO/Tqh3soOfo5s6cpwK61jcnowY2j+vLCqmJKj5/2uxyRFlFDaGeOVVTyzLJCrhmWogC7NjJrihd8t6zQ71JEWkQNoZ15YaUC7NrakLSuzLg8g9fWKPhOopsaQjty6HgFL67ayVdH92N0eg+/y4kp93nBd4+9V+B3KSIhU0NoR56uC7Ab7ncpMad3947cfm0mf9m4n80l5X6XIxISNYR2orj0BK+vDQTYDU5VgJ0f7poQCL6bs2i736WIhEQNoZ14bEmBAux81r1jB2ZOzGJ10WFWFZb6XY5Is6khtAMb937GO5sPcIcC7Hz3vSsHMqBHJx5dqOA7iT5qCFEuEGC3PRBgNy7T73JiXlJCPLOmBoLv3t58wO9yRJpFDSHKrSw8zN+Lj3DPxGF069jB73KEQPDdiL7dmLtYwXcSXdQQolhtgF1Gr05894pBfpcjnrg44+FpI9hTdorX1+7xuxyRoAXVEMws18zyzazIzGY38nqSmb3hvb7GzAbXe+0Rb3q+mU2tN72Hmb1lZtvNbJuZXRWOFYolCzbuZ9uBY8yaMlwBdhHmuuw0rhzSi6eXFir4TqJGk58iZhYPPAtMA0YCt5jZyAbDbgeOOueGAU8Ac7x5RwIzgAuBXOBX3vIAngIWOedGAGOAbS1fndhxuqqauUvyGdlPAXaRKBB8dwFHTp7hxVUKvpPoEMyflWOBIudcsXPuDDAfmN5gzHTgFe/xW8AkMzNv+nzn3Gnn3E6gCBhrZt2B8cBLAM65M865z1q+OrHjtTUKsIt0F2f0YNpFfXlhpYLvJDoE0xAGAHvrPS/xpjU6xjlXBZQDKeeZdwhQCvyHmX1sZi+aWaN3U5nZXWaWZ2Z5paW6thvgeEUlzywr8gLsUv0uR85j1tThVFTVME/BdxIFgmkIjf352fAC63ONOdf0BOBS4NfOuUuAk8CXzk0AOOeed87lOOdy0tKU3gmBALuyk2d4OHcEgR0xiVRD07ryncszeHXNHnYfUfCdRLZgGkIJkFHveTrQ8JvF68aYWQKQDJSdZ94SoMQ5t8ab/haBBiFNOHS8ghdW7eQrCrCLGvd7wXdzlyj4TiJbMA1hHZBlZplmlkjgJPGCBmMWALd5j28GljnnnDd9hncVUiaQBax1zh0E9ppZbQrbJOCTFq5LTHhmaZEC7KJM/eC7LfsUfCeRq8mG4J0TmAksJnAl0JvOua1m9nMzu8kb9hKQYmZFwAN4h3+cc1uBNwl82C8C7nbOVXvz3AO8amabgIuB/xO+1Wqfdh4+yetr93DL2IFkKsAuqij4TqJBQjCDnHPvAu82mPav9R5XAN86x7z/G/jfjUzfAOQ0p9hYN3dJPh3i47hn0jC/S5Fm6t6xA3dfP4z/9c42VhWW6tvsJCLpbqYosankM97ZdIA7x2XSu1tHv8uREPzjVYMY0KMTcxYp+E4ikxpCFKgNsOvVJZE7xw/xuxwJUVJCPA9OyWbLvmO8o+A7iUBqCFFgVeFhPtihALv2YPrFXvDdEgXfSeRRQ4hwZwfYDfS7HGmh+Djj4dwR7D5yivnrFHwnkUUNIcL9ZdN+PjlwjAcnDycpIb7pGSTiXTc8jSsyFXwnkUcNIYKdqaph7pJ8LujXnZvGKMCuvQgE343g8AkF30lkUUOIYK+t2c3eMgXYtUeXDOxZF3x3+ISC7yQyqCFEqOMVlTy9rIirh6YwXgF27dIXwXdFfpciAqghRKwXVu1UgF07NzStK9/OyeDVNbsVfCcRQQ0hApUeP82Lq4r5yqh+jMlQgF17dv8NWcTHGY8p+E4igBpCBHpmWSGnq2qYNVUBdu1dHy/4boGC7yQCqCFEmF2HT/Lamj3cMjZDAXYx4ocThtJDwXcSAdQQIkxtgN29k7L8LkXaSPeOHZh5/TBWFR5mdeFhv8uRGKaGEEE2l5Tz9qYD3KEAu5jzvSsVfCf+U0OIIHMWbadn5w7cpQC7mNOxQzwPTM5m875yBd+Jb9QQIsSqwlJWFx3mnolZCrCLUV+/ZADD+wSC7yqrFXwnbU8NIQLUBtil9+zErVcqwC5WxccZD08bHgi+W6vgO2l7aggR4C+b9rN1/zEenJKtALsYd/3w3ozN7MVTSws5qeA7aWNqCD47U1XDY0sKuKBfd6aPGeB3OeKzs4PvdvpdjsQYNQSfvb52D3vKTvFw7nAF2AkAlw7sSe6FfXl+5Q6OKPhO2pAago9OnK7i6aWFXDUkhQnZ+tJ1+cKsqcP5vLKaZxR8J21IDcFHL6ws5sjJM8yepgA7Oduw3l35zuWB4Ls9R075XY7ECDUEn5QeP80Lq4q5cVRfBdhJo+6blB0Ivnsv3+9SJEYE1RDMLNfM8s2syMxmN/J6kpm94b2+xswG13vtEW96vplNbTBfvJl9bGZvt3RFos282gC7KQqwk8b1Te7IP1+TyZ83KPhO2kaTDcHM4oFngWnASOAWMxvZYNjtwFHn3DDgCWCON+9IYAZwIZAL/MpbXq37gG0tXYlos/vISV5ds4cZl2cwJK2r3+VIBPvhhKEkd+rALxZrL0FaXzB7CGOBIudcsXPuDDAfmN5gzHTgFe/xW8AkCxwUnw7Md86dds7tBIq85WFm6cBXgBdbvhrRZe6SAjrEx3GfAuykCcmdAsF3KwtK+VuRgu+kdQXTEAYAe+s9L/GmNTrGOVcFlAMpTcz7JPAQEFP36G8uKecvG/dz+7WZ9O6uADtp2j9eNYj+yR15dKGC76R1BdMQGrv8peFv5bnGNDrdzL4KHHLOrW/yh5vdZWZ5ZpZXWlradLURri7AboIC7CQ4HTvE88CU4WzeV867WxR8J60nmIZQAmTUe54O7D/XGDNLAJKBsvPMew1wk5ntInAIaqKZ/b6xH+6ce945l+Ocy0lLi+5r9WsD7GZOzKK7AuykGb5RG3y3WMF30nqCaQjrgCwzyzSzRAIniRc0GLMAuM17fDOwzDnnvOkzvKuQMoEsYK1z7hHnXLpzbrC3vGXOue+FYX0iVk2NY86i7Qzo0YnvKcBOmik+zngodzi7jpxi/rq9Tc8gEoImG4J3TmAmsJjAFUFvOue2mtnPzewmb9hLQIqZFQEPALO9ebcCbwKfAIuAu51z1eFfjcj39uYDbNl3jFlTFWAnoZk4ojdjB/fiqb8q+E5ahwX+kI8OOTk5Li8vz+8ymu1MVQ03PL6CzonxvHvvOGUWScjW7z7KN3/9AQ9MztbXrErQzGy9cy6nqXG6U7kNzF/nBdhNG6FmIC1y2aCeTL2wD8+tUPCdhJ8aQiurDbC7ckgvrlOAnYTBT73gu3nLFXwn4aWG0MpeXFXM4RNnmD3tAgXYSVgM692Nb+dk8PsPd7O3TMF3Ej5qCK2o9PhpXlhZzLSL+nKxAuwkjO6/IZs4Mx5bokgLCR81hFY0b1khFVU1zJqqADsJr77JHfnnazP504b9bN2v4DsJDzWEVrL7yEleW7uH71yewVAF2Ekr+FFt8N0i7SVIeKghtJLHlhSQEBfH/bo0UFpJcqcO3H39UFYUlPKBgu8kDNQQWsGWfeUsUICdtIHvXzU4EHy3aDvRdE+RRCY1hFagADtpKx07xPMvk7PZVFLOu5sP+l2ORDk1hDBbXXiYVYWHufv6YQqwkzbxD5emk92nK79cvF3Bd9IiaghhVD/A7h+vGuR3ORIj4uOMh6aOYNeRU7yh4DtpATWEMHpn8wE27yvnwSkKsJO2NemC3lw+uCdPKvhOWkANIUzOVNUwd0k+I/p2Y/rFDb9QTqR1mRmzp43g8InTvLx6p9/lSJRSQwiT+ev2sPvIKR7OHUG8AuzEB5cN6sWUkX14bmWxgu8kJGoIYXDSC7C7IrMX1w1XgJ3456Hc4Zw6U8Wzy3f4XYpEITWEMHhx1U4vwG6EAuzEV8N6d+Nbl2Xwuw93KfhOmk0NoYUOnzjN8yt3MO2ivlwysKff5Yhw/+Qs4sx4/L0Cv0uRKKOG0ELzlhUpwE4iSr/kTvzTNZn8acM+Bd9Js6ghtMCeI6d4dc1uvp2jADuJLD+eMJTuHRV8J82jhtACj72XT3yccf8NCrCTyJLcuQM/uc4Lvtuh4DsJjhpCiLbsK+fPGwIBdn0UYCcR6LarB9MvuSNzFir4ToKjhhCiOYu206NzB344YajfpYg0qjb4bmNJOQu3KPhOmqaGEIK/FQUC7GYqwE4i3Dfrgu/yFXwnTVJDaKaaGsejCwMBdt+7UgF2Etlqg+92Hj7Jm3kKvpPzC6ohmFmumeWbWZGZzW7k9SQze8N7fY2ZDa732iPe9Hwzm+pNyzCz5Wa2zcy2mtl94Vqh1vbulkCA3QOTs+nYQQF2EvkmXdCbnEGB4LtTZxR8J+fWZEMws3jgWWAaMBK4xcxGNhh2O3DUOTcMeAKY4807EpgBXAjkAr/yllcFPOicuwC4Eri7kWVGnMrqGn65OBBg9/VLFGAn0aE2+K70uILv5PyC2UMYCxQ554qdc2eA+cD0BmOmA694j98CJlkgw2E6MN85d9o5txMoAsY65w445z4CcM4dB7YBEf8JO39tIMDuodzhCrCTqJIzuBeTR/bhNyuKKTt5xu9yJEIF0xAGAPUPPpbw5Q/vujHOuSqgHEgJZl7v8NIlwJrgy257J09X8dTSIsZm9uL64b39Lkek2R6aGgi+m7esyO9SJEIF0xAa+1O44UXN5xpz3nnNrCvwX8D9zrljjf5ws7vMLM/M8kpLS4Mot3W8tHonh0+cVoCdRK2sPt24+bJ0fv/hbgXfSaOCaQglQEa95+nA/nONMbMEIBkoO9+8ZtaBQDN41Tn3h3P9cOfc8865HOdcTlqaP9HSR06c5rkVO8i9sC+XKsBOotj9N2RjBo+/V6Cb1eRLgmkI64AsM8s0s0QCJ4kXNBizALjNe3wzsMwFftsWADO8q5AygSxgrXd+4SVgm3Pu8XCsSGt6ZlkRn1dWK8BOol7/Hp34wTWD+ePH+xj/y+X89z9t5r1PPuWEvnZTgISmBjjnqsxsJrAYiAdeds5tNbOfA3nOuQUEPtx/Z2ZFBPYMZnjzbjWzN4FPCFxZdLdzrtrMrgX+EdhsZhu8H/XfnHPvhnsFW6o2wO47l2cwrLcC7CT6PTh5OBk9O/N+fil/+Ggfv/9wDx3ijZxBvZgwPI0J2WmM6NtNh0ZjkEXTbmNOTo7Ly8tr0595//yPWbT1IO/Pup6+ycoskvblTFUNebvLWFFQyor8UrYfPA5A725JTMhOY3x2GuOyUunROdHnSqUlzGy9cy6nqXFN7iHEsi37yvnThv385LqhagbSLiUmxHGkQLAeAAANrElEQVT10FSuHprKI9Mu4NNjFYHmUFDKkk8+5T/XlxBnMCajBxOyA3sPo9N76LLrdkp7COfx/ZfXsqnkM1b89HqSOymzSGJLVXUNG0vKWVFQysqCUjaWfIZz0KNzB8ZlpXl7EKn07qY/liKd9hBa6IOiw6wsKOVnN16gZiAxKSE+jssG9eSyQT15YHI2ZSfPsLroMCvyA3sQf9kYuNhwZL/udeceLh3Yk8QERaRFK+0hNMI5x/Rn/8bh46dZNus6ZRaJNFBT49h28FjduYf1u49SVePokhjP1cNS6w4vZfTq7HepgvYQWuTdzQfZVFLO3G+NUTMQaURcnHFh/2Qu7J/MT64bxvGKSj7YcaSuQbz3yacADEnrUtccrhySovdThNMeQgOV1TVMfnwFSQnxvHvfOJ08E2km5xzFh0/WHVr6sPgIp6tqSEqI44ohKXUNYmhaF13a2ka0hxCi+ev2suvIKV7+QY6agUgIzIyhaV0ZmtaVf742k4rKatbsLPMaxCH+/e1P+HdgQI9Odecerh6aQjd92ZTv1BDqOXm6iqf+WsjYwQqwEwmXjh3i6/YKYCR7y06xsjBwaGnBhv28tmYPCXHGZYN6Mt4bN7Jfd+L0B1mb0yGjep5ZWshj7xXwXz++mssGKbNIpLVVVtfw0e6jdfc+bN0fyLhM7ZrE+OzAyelxWWn06qIb41pCh4ya6ciJ0zy3spipF/ZRMxBpIx3iA+cVrhiSwkO5Izh0vIJVBYdZUVDK8u2H+MNH+zCD0elf3Bh3cYZujGstagieecuLOHWmip9OHeF3KSIxq3e3jnzzsnS+eVk61TWOzfvK6849zFtWyNNLC0nu1IFrs764tLVPd90YFy5qCMDeslP8/sPdfDtHAXYikSI+zrg4owcXZ/Tgvhuy+OzU2TfGvbPpAAAj+narOzmdM6iXboxrATUEAtnwcWbcf0O236WIyDn06JzIV0f356uj++OcY/vB43WxGi+v3slzK4rpnBjP1UNrL23tzcAU3RjXHDHfELbuL+dPG/bxowkKsBOJFmbGBf26c0G/7vxowlBOnq7i796Nce8XHOKv2w4BW8lMPfvGuE6JujHufGK+IfxiUT7dO3bgRxOG+l2KiISoS1ICN4zsww0j++CcY9eRU6zIP8SKglLmr9vD//tgF4kJcVyR2auuQQzr3VU3xjUQ05edflB0mO++uIaf3XgBd44fErblikjkqKisZt2usrpzD4WHTgDQP7lj3X0PVw9LbdchlsFedhqzDcE5x9ef/RulCrATiSn7P/ucld59D6sLD3P8dBXxccalA3vUnXu4sH/7ujFODaEJ724+wE9e/Yhf3jyab+VkhGWZIhJdKqtr2LD3s7q9h837ygFI6ZJYt/cwLiuVlK5JPlfaMmoI51FZXcOUJ1bSId5YeN943eQiIgAcPnGaVV6sxsrCw5SdPIMZjBqQfNaNcQnx0XVpq+5UPo831u1l5+GTvHSbAuxE5AupXZP4xiXpfOOSdGpqHFv2l9cdXvrV+zt4ZlkR3TomMM67MW58dhr9kjv5XXbYxFxDOHWmiqeWFnL54J5MHKEAOxFpXFycMTq9B6PTezBzYhbln1fyQdHhutyldzcfBCC7T9e6cw+XZ/YkKSF6z0fGXEN4efVOSo+f5jffu1SXnIlI0JI7dWDaqH5MG9UP5xyFh07UnXt45YPdvLBqJ506xHPV0C++82Fwahe/y26WmGoIZSfP8JsVxUwZ2YfLBvXyuxwRiVJmRnafbmT36cad44dw6kwVHxYfqWsQy7YfAmBQSuezbozrkhTZH7mRXV2YzVsWCLB7KHe436WISDvSOTGBiSP6MHFEHwB2HT5Z950P/5lXwm//vpvE+Dguz+xZd3gpu0/k3RgX1FVGZpYLPAXEAy865x5t8HoS8FvgMuAI8B3n3C7vtUeA24Fq4F7n3OJgltmYllxltLfsFJMeW8E/XDqAR785OqRliIg01+mqatbv+uI7H7YfPA5A3+4d605MXzssleTOrXdjXNguOzWzeKAAmAyUAOuAW5xzn9Qb8xNgtHPuR2Y2A/iGc+47ZjYSeB0YC/QH/grUJsidd5mNaUlDeOCNDbyz+QArfnq9MotExDcHyyvqrlxaVVjKsYoq4gwuGdiz7vDSqAHJYb0xLpyXnY4Fipxzxd6C5wPTgfof3tOBf/MevwXMs8C+0HRgvnPuNLDTzIq85RHEMsPmk/3H+OOGffxwvALsRMRffZM78u3LM/j25RlUVdewseSLG+Oe+GsBj79XQK8uiXWXto7LSiOtW9vcGBdMQxgA7K33vAS44lxjnHNVZlYOpHjTP2ww7wDvcVPLDJtfLN5O944d+LEC7EQkgiTEx3HZoF5cNqgXD0wZTtnJM4Eb47xY7z9v2A/ARQO688o/jW31O6aDaQiN7bc0PM50rjHnmt7YbX6NHrsys7uAuwAGDhx47irPobrGkd2nGxOy01r1GJ2ISEv16pLI9IsHMP3iAdTUOD45cIwVBaVsKvmsTb5XOpiGUALUD/tJB/afY0yJmSUAyUBZE/M2tUwAnHPPA89D4BxCEPWeJT7O+G83XtDc2UREfBUXZ1w0IJmLBiS33c8MYsw6IMvMMs0sEZgBLGgwZgFwm/f4ZmCZC5ytXgDMMLMkM8sEsoC1QS5TRETaUJN7CN45gZnAYgKXiL7snNtqZj8H8pxzC4CXgN95J43LCHzA4417k8DJ4irgbudcNUBjywz/6omISLBiMu1URCSWBHvZaXRluIqISKtRQxAREUANQUREPGoIIiICqCGIiIgnqq4yMrNSYHeIs6cCh8NYTmuKplohuuqNplohuuqNplohuuptaa2DnHNpTQ2KqobQEmaWF8xlV5EgmmqF6Ko3mmqF6Ko3mmqF6Kq3rWrVISMREQHUEERExBNLDeF5vwtohmiqFaKr3miqFaKr3miqFaKr3japNWbOIYiIyPnF0h6CiIicR7tvCGaWa2b5ZlZkZrP9rgfAzDLMbLmZbTOzrWZ2nze9l5m9Z2aF3v89velmZk9767DJzC71oeZ4M/vYzN72nmea2Rqv1je8GHO8qPM3vFrXmNlgH2rtYWZvmdl2bxtfFanb1sz+xfsd2GJmr5tZx0jatmb2spkdMrMt9aY1e1ua2W3e+EIzu62xn9VKtf7S+z3YZGZ/NLMe9V57xKs138ym1pveJp8ZjdVb77VZZubMLNV73jbb1jnXbv8RiNbeAQwBEoGNwMgIqKsfcKn3uBtQAIwEfgHM9qbPBuZ4j28EFhL4BrorgTU+1PwA8Brwtvf8TWCG9/g3wI+9xz8BfuM9ngG84UOtrwB3eI8TgR6RuG0JfJ3sTqBTvW36g0jatsB44FJgS71pzdqWQC+g2Pu/p/e4ZxvVOgVI8B7PqVfrSO/zIAnI9D4n4tvyM6Oxer3pGQS+GmA3kNqW27ZNfvH9+gdcBSyu9/wR4BG/62qkzj8Dk4F8oJ83rR+Q7z1+Dril3vi6cW1UXzqwFJgIvO39Uh6u90ar287eL/JV3uMEb5y1Ya3dvQ9ZazA94rYtX3wXeS9vW70NTI20bQsMbvAh26xtCdwCPFdv+lnjWrPWBq99A3jVe3zWZ0Httm3rz4zG6gXeAsYAu/iiIbTJtm3vh4xq33C1SrxpEcPb7b8EWAP0cc4dAPD+7+0N83s9ngQeAmq85ynAZ865qkbqqavVe73cG99WhgClwH94h7heNLMuROC2dc7tA+YCe4ADBLbVeiJ329Zq7rb0+/e31j8T+CsbIrRWM7sJ2Oec29jgpTapt703BGtkWsRcVmVmXYH/Au53zh0739BGprXJepjZV4FDzrn1Qdbj9zZPILAb/mvn3CXASQKHNc7Fz23bE5hO4JBFf6ALMO089fi9bZtyrvp8r9vMfkbgWxtfrZ3UyDBfazWzzsDPgH9t7OVGpoW93vbeEEoIHI+rlQ7s96mWs5hZBwLN4FXn3B+8yZ+aWT/v9X7AIW+6n+txDXCTme0C5hM4bPQk0MPMar+CtX49dbV6rycT+FrVtlIClDjn1njP3yLQICJx294A7HTOlTrnKoE/AFcTudu2VnO3pa/vQ+9E61eBW513XOU8NflZ61ACfxxs9N5v6cBHZtb3PHWFtd723hDWAVneVRuJBE7ELfC5JszMCHwP9Tbn3OP1XloA1F4lcBuBcwu107/vXWlwJVBeu8ve2pxzjzjn0p1zgwlsv2XOuVuB5cDN56i1dh1u9sa32V+DzrmDwF4zG+5NmkTgO70jbtsSOFR0pZl19n4namuNyG1bT3O35WJgipn19PaKpnjTWp2Z5QIPAzc55041WIcZ3pVbmUAWsBYfPzOcc5udc72dc4O991sJgYtPDtJW27a1TpZEyj8CZ+cLCFw58DO/6/FqupbAbt0mYIP370YCx4OXAoXe/7288QY8663DZiDHp7qv44urjIYQeAMVAf8JJHnTO3rPi7zXh/hQ58VAnrd9/0Tg6ouI3LbA/wS2A1uA3xG46iViti3wOoHzG5UEPqBuD2VbEjh+X+T9+6c2rLWIwDH22vfZb+qN/5lXaz4wrd70NvnMaKzeBq/v4ouTym2ybXWnsoiIAO3/kJGIiARJDUFERAA1BBER8aghiIgIoIYgIiIeNQQREQHUEERExKOGICIiAPx/ZUbbgdhF3YcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs = get_triangular_lr2(1e-5, 0.012, 700)\n",
    "plt.plot(lrs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_triangular_policy(model, train_dl, valid_dl, lr_low=1e-5, lr_high=0.012):\n",
    "    idx = 0\n",
    "    epochs = 4\n",
    "    stepesize = 2*len(train_dl)\n",
    "    lrs = get_triangular_lr2(lr_low, lr_high, stepesize)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        sum_loss = 0\n",
    "        for i, (x, y) in enumerate(train_dl):\n",
    "            optim = get_optimizer(model, lr = lrs[idx], wd =0)\n",
    "            batch = x.shape[0]\n",
    "            x = Variable(x).cuda().float()\n",
    "            y = Variable(y).cuda().float()\n",
    "        \n",
    "            out = model(x)\n",
    "            loss = F.l1_loss(out, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            idx += 1\n",
    "            total += batch\n",
    "            sum_loss += batch*(loss.data[0])\n",
    "        print(\"train loss\", sum_loss/total)\n",
    "        val_loss(model, valid_dl)\n",
    "    return sum_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loss(model, valid_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for i, (x, y) in enumerate(valid_dl):\n",
    "        batch = x.shape[0]\n",
    "        x = Variable(x).cuda().float()\n",
    "        y = Variable(y).cuda().float()\n",
    "        out = model(x)\n",
    "        loss = F.l1_loss(out, y)\n",
    "        sum_loss += batch*(loss.data[0])\n",
    "        total += batch\n",
    "    print(\"val loss\", sum_loss/total)\n",
    "    return sum_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 4, 6, 3]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def training_loop(model, train_dl, valid_dl, steps=3, lr_low=1e-5, lr_high=0.012):\n",
    "    for i in range(steps):\n",
    "        start = datetime.now() \n",
    "        loss = train_triangular_policy(model, train_dl, valid_dl, lr_low, lr_high)\n",
    "        end = datetime.now()\n",
    "        t = 'Time elapsed {}'.format(end - start)\n",
    "        print(\"----End of step\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 45.623061737535444\n",
      "val loss 72.64290736876812\n",
      "train loss 33.96514722431057\n",
      "val loss 35.516014837254424\n",
      "train loss 31.221370589261674\n",
      "val loss 31.886717078612023\n",
      "train loss 29.323143192154763\n",
      "val loss 28.05830501281109\n",
      "----End of step Time elapsed 0:23:41.516299\n",
      "train loss 29.417924961706778\n",
      "val loss 36.223304899791984\n",
      "train loss 25.385740252171626\n",
      "val loss 26.900981534009507\n",
      "train loss 19.42219648494488\n",
      "val loss 18.386266045112823\n",
      "train loss 17.54091583639132\n",
      "val loss 15.574896247494703\n",
      "----End of step Time elapsed 0:23:19.447646\n",
      "train loss 19.2133062663075\n",
      "val loss 35.09704175097521\n",
      "train loss 21.49387811907858\n",
      "val loss 21.830231925970406\n",
      "train loss 17.544949240698216\n",
      "val loss 16.363899745230256\n",
      "train loss 15.900487466283355\n",
      "val loss 14.082288673999855\n",
      "----End of step Time elapsed 0:22:50.967878\n",
      "train loss 17.37976834776144\n",
      "val loss 50.26269467117104\n",
      "train loss 19.754191285283753\n",
      "val loss 40.49533951991518\n",
      "train loss 16.396642890814476\n",
      "val loss 17.513908811262162\n",
      "train loss 14.966848538455743\n",
      "val loss 13.136624055661255\n",
      "----End of step Time elapsed 0:23:22.562283\n",
      "train loss 16.32030310183217\n",
      "val loss 28.00123822169111\n",
      "train loss 18.98807388836523\n",
      "val loss 48.42753493984383\n",
      "train loss 15.691107584045573\n",
      "val loss 15.076681931939605\n",
      "train loss 14.388662994079256\n",
      "val loss 12.537261620293904\n",
      "----End of step Time elapsed 0:23:20.204403\n"
     ]
    }
   ],
   "source": [
    "training_loop(model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 14.263382120970894\n",
      "val loss 13.272155406265199\n",
      "train loss 14.783731592718846\n",
      "val loss 16.94928747959122\n",
      "train loss 13.919891743844772\n",
      "val loss 12.100011907240212\n",
      "train loss 13.62299452150337\n",
      "val loss 12.008184200047879\n",
      "----End of step Time elapsed 0:23:20.455724\n",
      "train loss 13.848923162043844\n",
      "val loss 13.725671961040936\n",
      "train loss 14.34631811590589\n",
      "val loss 15.342303196652177\n",
      "train loss 13.559383945501008\n",
      "val loss 11.911217498174269\n",
      "train loss 13.206149436730627\n",
      "val loss 11.735988587448277\n",
      "----End of step Time elapsed 0:23:44.948994\n",
      "train loss 13.43918061707771\n",
      "val loss 14.466383988473455\n",
      "train loss 14.16615095959982\n",
      "val loss 15.315539631552397\n",
      "train loss 13.232009625668828\n",
      "val loss 11.840880841702152\n",
      "train loss 12.8478607754722\n",
      "val loss 11.540226731387326\n",
      "----End of step Time elapsed 0:23:41.456009\n"
     ]
    }
   ],
   "source": [
    "training_loop(model, 3, lr_low=1e-5, lr_high=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 13.152737889255354\n",
      "val loss 16.778002031826954\n",
      "train loss 13.935005700698113\n",
      "val loss 15.376870121301685\n",
      "train loss 12.927443265465918\n",
      "val loss 11.50160617737615\n",
      "train loss 12.524969114374862\n",
      "val loss 11.414389673062113\n",
      "----End of step Time elapsed 0:23:13.271924\n",
      "train loss 12.901719939426313\n",
      "val loss 12.369769128894731\n",
      "train loss 13.652108467231363\n",
      "val loss 12.843103706033148\n",
      "train loss 12.782786859819916\n",
      "val loss 11.491814626199115\n",
      "train loss 12.276938548160386\n",
      "val loss 11.251593628353207\n",
      "----End of step Time elapsed 0:23:49.900147\n",
      "train loss 12.65916109193788\n",
      "val loss 13.824012242074433\n",
      "train loss 13.296641189570659\n",
      "val loss 12.547889372169925\n",
      "train loss 12.299246762408316\n",
      "val loss 11.549489819937335\n",
      "train loss 11.898699179762879\n",
      "val loss 11.10967648965229\n",
      "----End of step Time elapsed 0:23:43.220666\n"
     ]
    }
   ],
   "source": [
    "training_loop(model, 3, lr_low=1e-5, lr_high=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 12.36981978328061\n",
      "val loss 12.233699474138083\n",
      "train loss 13.099694195960154\n",
      "val loss 13.725741510444175\n",
      "train loss 12.055306261743022\n",
      "val loss 10.920661044442209\n",
      "train loss 11.55018129045326\n",
      "val loss 10.963666866735085\n",
      "----End of step Time elapsed 0:23:46.093495\n",
      "train loss 11.964053231597667\n",
      "val loss 12.50550051287561\n",
      "train loss 12.724950222681223\n",
      "val loss 12.561069826584406\n",
      "train loss 11.707698583437484\n",
      "val loss 10.982128026842787\n",
      "train loss 11.227378725733791\n",
      "val loss 10.844734530709832\n",
      "----End of step Time elapsed 0:23:51.726653\n",
      "train loss 11.646148155319027\n",
      "val loss 15.242850980524219\n",
      "train loss 12.57666299884035\n",
      "val loss 11.134821988589039\n",
      "train loss 11.53858673439022\n",
      "val loss 10.762744333326202\n",
      "train loss 11.082182285393156\n",
      "val loss 10.677097285011286\n",
      "----End of step Time elapsed 0:23:49.416767\n"
     ]
    }
   ],
   "source": [
    "training_loop(model, 3, lr_low=1e-5, lr_high=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 11.49689058842065\n",
      "val loss 12.739510871984011\n",
      "train loss 12.44505853092414\n",
      "val loss 11.546216138859386\n",
      "train loss 11.192747556958826\n",
      "val loss 10.864729668770774\n",
      "train loss 10.647953678493705\n",
      "val loss 10.554042250461563\n",
      "----End of step Time elapsed 0:23:51.858458\n",
      "train loss 11.073326064143926\n",
      "val loss 12.313348615859635\n",
      "train loss 12.125249311205346\n",
      "val loss 11.755949643563884\n",
      "train loss 11.003178162905137\n",
      "val loss 10.867132850907135\n",
      "train loss 10.48366370037108\n",
      "val loss 10.493077545861796\n",
      "----End of step Time elapsed 0:23:51.417922\n",
      "train loss 10.892317548168226\n",
      "val loss 11.862814212771847\n",
      "train loss 12.00909862259309\n",
      "val loss 13.837489718014432\n",
      "train loss 10.805907002264805\n",
      "val loss 10.558478773632851\n",
      "train loss 10.300284773710993\n",
      "val loss 10.305330107081607\n",
      "----End of step Time elapsed 0:23:48.976277\n"
     ]
    }
   ],
   "source": [
    "training_loop(model, 3, lr_low=1e-5, lr_high=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 10.841456693350022\n",
      "val loss 10.870390486660503\n",
      "train loss 11.867366835954984\n",
      "val loss 11.279734305216149\n",
      "train loss 10.523179212591554\n",
      "val loss 10.495949729296255\n",
      "train loss 10.02298401836267\n",
      "val loss 10.370062710839921\n",
      "----End of step Time elapsed 0:23:51.382056\n",
      "train loss 10.46892860923795\n",
      "val loss 11.153744964539106\n",
      "train loss 11.679356946216394\n",
      "val loss 11.0543840780417\n",
      "train loss 10.39783201993711\n",
      "val loss 10.754701489592998\n",
      "train loss 9.906225928916179\n",
      "val loss 10.21090637890712\n",
      "----End of step Time elapsed 0:23:47.217277\n",
      "train loss 10.367800475419342\n",
      "val loss 14.13131189459756\n",
      "train loss 11.514203224632073\n",
      "val loss 13.634746556051377\n",
      "train loss 10.346584185463124\n",
      "val loss 10.485832981227599\n",
      "train loss 9.832593202283732\n",
      "val loss 10.257340687593707\n",
      "----End of step Time elapsed 0:23:49.101725\n"
     ]
    }
   ],
   "source": [
    "training_loop(model, 3, lr_low=1e-5, lr_high=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PATH/\"model025.pth\"\n",
    "save_model(model, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 10.320222003878918\n",
      "val loss 12.01687013866597\n",
      "train loss 11.499148212583725\n",
      "val loss 13.043459376487156\n",
      "train loss 10.13222805332861\n",
      "val loss 10.655708781884456\n",
      "train loss 9.634237906684463\n",
      "val loss 10.119181758538774\n",
      "----End of step Time elapsed 0:23:25.615686\n",
      "train loss 10.123079956316335\n",
      "val loss 11.283315898311413\n",
      "train loss 11.363553073363919\n",
      "val loss 14.866108746872735\n",
      "train loss 10.005506338739032\n",
      "val loss 10.578337949197696\n",
      "train loss 9.549798419282013\n",
      "val loss 10.159578586551143\n",
      "----End of step Time elapsed 0:23:37.564133\n",
      "train loss 10.074453023985576\n",
      "val loss 11.844373743842274\n",
      "train loss 11.26691515337713\n",
      "val loss 11.221265520584385\n",
      "train loss 9.874618237053324\n",
      "val loss 10.219159203422722\n",
      "train loss 9.431668565417201\n",
      "val loss 10.049855900793961\n",
      "----End of step Time elapsed 0:23:37.922840\n"
     ]
    }
   ],
   "source": [
    "training_loop(model, 3, lr_low=1e-5, lr_high=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PATH/\"model004.pth\"\n",
    "save_model(model, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 9.898709399551995\n",
      "val loss 16.53361714519649\n",
      "train loss 11.093989599777858\n",
      "val loss 11.84160546099355\n",
      "train loss 9.895924568459847\n",
      "val loss 10.052780738999974\n",
      "train loss 9.334162484383675\n",
      "val loss 9.951132005968327\n",
      "----End of step Time elapsed 0:23:46.645139\n",
      "train loss 9.890184978697224\n",
      "val loss 10.980202742931297\n",
      "train loss 10.969015524708512\n",
      "val loss 11.539697710622596\n",
      "train loss 9.689701637081283\n",
      "val loss 10.179851061573679\n",
      "train loss 9.186172535261369\n",
      "val loss 9.953562956589918\n",
      "----End of step Time elapsed 0:22:45.845982\n",
      "train loss 9.767673405656636\n",
      "val loss 11.647481587082503\n",
      "train loss 10.833313521030625\n",
      "val loss 11.349242242908403\n",
      "train loss 9.69085806563891\n",
      "val loss 10.096809537707578\n",
      "train loss 9.204073897531996\n",
      "val loss 9.825388880403716\n",
      "----End of step Time elapsed 0:22:42.097060\n"
     ]
    }
   ],
   "source": [
    "training_loop(model, 3, lr_low=1e-5, lr_high=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PATH/\"mode982.pth\"\n",
    "save_model(model, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 9.659912690521763\n",
      "val loss 13.264713642807067\n",
      "train loss 10.738028198394941\n",
      "val loss 11.036421859765412\n",
      "train loss 9.499015757250401\n",
      "val loss 10.036042030419177\n",
      "train loss 9.082798625376242\n",
      "val loss 9.778036493426935\n",
      "----End of step Time elapsed 0:27:03.286398\n",
      "train loss 9.543949227205932\n",
      "val loss 11.826622597666415\n",
      "train loss 10.771479258677664\n",
      "val loss 11.87407975949341\n",
      "train loss 9.534867190549036\n",
      "val loss 10.036091538292373\n",
      "train loss 8.96284408410532\n",
      "val loss 9.759170072405798\n",
      "----End of step Time elapsed 0:26:03.244225\n",
      "train loss 9.410974728102868\n",
      "val loss 10.181903873144115\n",
      "train loss 10.636254415626462\n",
      "val loss 11.126130443636526\n",
      "train loss 9.444923675382842\n",
      "val loss 10.007536090999062\n",
      "train loss 9.027386239882041\n",
      "val loss 9.734913206592049\n",
      "----End of step Time elapsed 0:23:47.838629\n",
      "train loss 9.33647258625074\n",
      "val loss 10.632103466968703\n",
      "train loss 10.590949359031342\n",
      "val loss 11.436139217924259\n",
      "train loss 9.209595547049473\n",
      "val loss 9.935904312285047\n",
      "train loss 8.773063318995728\n",
      "val loss 9.719012035820619\n",
      "----End of step Time elapsed 0:23:44.138083\n",
      "train loss 9.203109389384611\n",
      "val loss 13.40906114676564\n",
      "train loss 10.509925258302609\n",
      "val loss 11.36800657842577\n",
      "train loss 9.253915989029123\n",
      "val loss 10.203709000353772\n",
      "train loss 8.747805304730042\n",
      "val loss 9.716702843166173\n",
      "----End of step Time elapsed 0:23:32.565712\n"
     ]
    }
   ],
   "source": [
    "training_loop(model, 5, lr_low=1e-5, lr_high=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PATH/\"mode971.pth\"\n",
    "save_model(model, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 8.78742747193865\n",
      "val loss 10.88776768284313\n",
      "train loss 9.114739328513522\n",
      "val loss 10.209122250895005\n",
      "train loss 8.679072800990859\n",
      "val loss 9.860550794972019\n",
      "train loss 8.453682288174493\n",
      "val loss 9.607586841749635\n",
      "----End of step Time elapsed 0:23:58.539779\n",
      "train loss 8.579691422305833\n",
      "val loss 10.062300491484265\n",
      "train loss 8.87156249269502\n",
      "val loss 10.06594440390627\n",
      "train loss 8.606406032725975\n",
      "val loss 9.73991787840127\n",
      "train loss 8.401609366896462\n",
      "val loss 9.60187570222496\n",
      "----End of step Time elapsed 0:23:16.432014\n",
      "train loss 8.478819820356033\n",
      "val loss 10.873824326789729\n",
      "train loss 8.77759507820431\n",
      "val loss 9.845678193336626\n",
      "train loss 8.483425294836588\n",
      "val loss 9.665561939212276\n",
      "train loss 8.391216542749389\n",
      "val loss 9.549710633733932\n",
      "----End of step Time elapsed 0:22:39.185608\n",
      "train loss 8.468748176212483\n",
      "val loss 10.361373977751887\n",
      "train loss 8.790247539921433\n",
      "val loss 9.98481042530686\n",
      "train loss 8.374656195351786\n",
      "val loss 9.691897973855463\n",
      "train loss 8.254242630942581\n",
      "val loss 9.553799357705415\n",
      "----End of step Time elapsed 0:22:46.188615\n",
      "train loss 8.385336730628836\n",
      "val loss 10.794609503175039\n",
      "train loss 8.696423900648487\n",
      "val loss 10.931374211806707\n",
      "train loss 8.326939146775581\n",
      "val loss 9.654655846029307\n",
      "train loss 8.229244492313697\n",
      "val loss 9.512078171018379\n",
      "----End of step Time elapsed 0:23:28.712429\n"
     ]
    }
   ],
   "source": [
    "training_loop(model, 5, lr_low=1e-5, lr_high=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PATH/\"mode951.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with sizes and bigger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Darknet([1, 3, 4, 6, 4]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training with different sizes\n",
    "size=128\n",
    "train_ds = BoneAgeDataset(train, sz=size)\n",
    "valid_ds = BoneAgeDataset(valid, transforms=False, sz=size)\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs, losses = LR_range_finder(model2, train_dl=train_dl, lr_low=1e-6, lr_high=1, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lrs[3:70], losses[3:70])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs[17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a bigger model using different image sizes\n",
    "First training with size=(128, 128). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 48.90728611473003\n",
      "val loss 93.56412246546795\n",
      "train loss 34.57827247969754\n",
      "val loss 33.69706777804055\n",
      "train loss 28.725175256924796\n",
      "val loss 35.52290987684838\n",
      "train loss 25.588178727090128\n",
      "val loss 25.563021621280583\n",
      "----End of step Time elapsed 0:10:05.260037\n",
      "train loss 26.756245738156238\n",
      "val loss 35.534808537772875\n",
      "train loss 28.169966735597857\n",
      "val loss 26.78992904886946\n",
      "train loss 22.711112914821275\n",
      "val loss 21.575234128027848\n",
      "train loss 20.80687678156836\n",
      "val loss 19.726066297429785\n",
      "----End of step Time elapsed 0:10:00.626601\n",
      "train loss 22.25122504887017\n",
      "val loss 26.30180537369779\n",
      "train loss 25.262563995828664\n",
      "val loss 29.938739395444117\n",
      "train loss 20.703193041434407\n",
      "val loss 19.034121057327354\n",
      "train loss 18.44051714791381\n",
      "val loss 17.18224536342152\n",
      "----End of step Time elapsed 0:10:01.979604\n"
     ]
    }
   ],
   "source": [
    "training_loop(model2, train_dl, valid_dl, steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=300\n",
    "train_ds = BoneAgeDataset(train, sz=size)\n",
    "valid_ds = BoneAgeDataset(valid, transforms=False, sz=size)\n",
    "batch_size = 20\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 26.044993554651352\n",
      "val loss 40.75597686223053\n",
      "train loss 23.944002681780955\n",
      "val loss 57.47501281024349\n",
      "train loss 17.839883319213094\n",
      "val loss 15.296197219655024\n",
      "train loss 15.979709047878423\n",
      "val loss 14.365781182055432\n",
      "----End of step Time elapsed 0:17:45.754115\n",
      "train loss 17.707598087962264\n",
      "val loss 32.35183999578127\n",
      "train loss 20.0995422680511\n",
      "val loss 17.7226585596918\n",
      "train loss 16.515401925945177\n",
      "val loss 15.158796330088192\n",
      "train loss 14.779526117169617\n",
      "val loss 13.165533575531418\n",
      "----End of step Time elapsed 0:17:58.419137\n",
      "train loss 16.35949897009904\n",
      "val loss 32.226625985516904\n",
      "train loss 19.121577081104633\n",
      "val loss 14.921346455694286\n",
      "train loss 15.567782258831459\n",
      "val loss 13.205089519177044\n",
      "train loss 14.122914174654898\n",
      "val loss 12.474478208101713\n",
      "----End of step Time elapsed 0:17:53.501802\n"
     ]
    }
   ],
   "source": [
    "training_loop(model2, train_dl, valid_dl, steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=400\n",
    "train_ds = BoneAgeDataset(train, sz=size)\n",
    "valid_ds = BoneAgeDataset(valid, transforms=False, sz=size)\n",
    "batch_size = 16\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 16.704957682616836\n",
      "val loss 60.258662496834496\n",
      "train loss 18.999557165031874\n",
      "val loss 25.099690439585746\n",
      "train loss 15.150926884225722\n",
      "val loss 14.32799159450062\n",
      "train loss 14.049556574508118\n",
      "val loss 12.061274733456424\n",
      "----End of step Time elapsed 0:23:55.504077\n",
      "train loss 15.559061320381721\n",
      "val loss 26.467239134846746\n",
      "train loss 17.552685243710734\n",
      "val loss 14.003673973960785\n",
      "train loss 14.85655387086667\n",
      "val loss 12.284045111272754\n",
      "train loss 13.738901040660153\n",
      "val loss 11.946764532296456\n",
      "----End of step Time elapsed 0:23:50.679160\n",
      "train loss 15.241433451203811\n",
      "val loss 39.308067379254936\n",
      "train loss 17.53291383055703\n",
      "val loss 16.993585914775174\n",
      "train loss 14.453696711163346\n",
      "val loss 13.926190339223815\n",
      "train loss 13.261515430681461\n",
      "val loss 11.630616101832922\n",
      "----End of step Time elapsed 0:23:50.308037\n"
     ]
    }
   ],
   "source": [
    "training_loop(model2, train_dl, valid_dl, steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 14.61832049580889\n",
      "val loss 55.65187916161993\n",
      "train loss 16.87780511096845\n",
      "val loss 27.70183369321165\n",
      "train loss 14.023264554957944\n",
      "val loss 12.319073411607251\n",
      "train loss 12.694166431224716\n",
      "val loss 11.279602157417315\n",
      "----End of step Time elapsed 0:23:48.021861\n",
      "train loss 14.11314756828442\n",
      "val loss 24.021565754199955\n",
      "train loss 16.709299003112562\n",
      "val loss 19.544616812661374\n",
      "train loss 13.548484523677534\n",
      "val loss 12.343640225355253\n",
      "train loss 12.368803217692493\n",
      "val loss 10.90747644180915\n",
      "----End of step Time elapsed 0:23:48.558980\n",
      "train loss 13.747964949753891\n",
      "val loss 23.246584095905735\n",
      "train loss 16.196004100190436\n",
      "val loss 32.43624926634387\n",
      "train loss 13.514724510254124\n",
      "val loss 11.265589320782533\n",
      "train loss 12.175860290686147\n",
      "val loss 10.807946668741346\n",
      "----End of step Time elapsed 0:23:49.148442\n"
     ]
    }
   ],
   "source": [
    "training_loop(model2, train_dl, valid_dl, steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 13.217544223782754\n",
      "val loss 13.720398756930225\n",
      "train loss 15.860890428662383\n",
      "val loss 12.165713792750989\n",
      "train loss 13.175634879661345\n",
      "val loss 11.24549870411618\n",
      "train loss 11.742005138073937\n",
      "val loss 10.713178320029348\n",
      "----End of step Time elapsed 0:25:18.767624\n",
      "train loss 13.076373187528397\n",
      "val loss 13.22388572602117\n",
      "train loss 15.676575249611345\n",
      "val loss 14.120502409908527\n",
      "train loss 12.858752071544334\n",
      "val loss 10.59982015971245\n",
      "train loss 11.394614895007598\n",
      "val loss 10.486111295308318\n",
      "----End of step Time elapsed 0:24:48.956338\n",
      "train loss 12.616183464325553\n",
      "val loss 39.17918028139482\n",
      "train loss 15.163319795854099\n",
      "val loss 15.489150734008257\n",
      "train loss 12.540265934899065\n",
      "val loss 10.939147332847165\n",
      "train loss 11.20275534831068\n",
      "val loss 10.450658135712855\n",
      "----End of step Time elapsed 0:24:32.771247\n"
     ]
    }
   ],
   "source": [
    "training_loop(model2, train_dl, valid_dl, steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 10.889478781967927\n",
      "val loss 10.830256437139791\n",
      "train loss 11.110302451344804\n",
      "val loss 10.352339962377707\n",
      "train loss 10.755905138889736\n",
      "val loss 10.19043447931639\n",
      "train loss 10.731803673040833\n",
      "val loss 10.15784996298927\n",
      "----End of step Time elapsed 0:23:50.630481\n",
      "train loss 10.774833302417989\n",
      "val loss 10.522304380630143\n",
      "train loss 10.792620678441425\n",
      "val loss 10.416687793716564\n",
      "train loss 10.429525662725704\n",
      "val loss 10.081736478419836\n",
      "train loss 10.372804753453542\n",
      "val loss 10.056434363623067\n",
      "----End of step Time elapsed 0:23:52.514577\n",
      "train loss 10.502904778441868\n",
      "val loss 10.192705640709656\n",
      "train loss 10.48187693799023\n",
      "val loss 10.354615402070422\n",
      "train loss 10.249422290963453\n",
      "val loss 10.21752375860615\n",
      "train loss 10.10243898086026\n",
      "val loss 10.047810630889094\n",
      "----End of step Time elapsed 0:23:49.094220\n",
      "train loss 10.182357430966148\n",
      "val loss 10.350297017290325\n",
      "train loss 10.398201023838732\n",
      "val loss 10.075562195774111\n",
      "train loss 10.046455453911815\n",
      "val loss 10.048704160951981\n",
      "train loss 9.90829246828017\n",
      "val loss 10.044402466606847\n",
      "----End of step Time elapsed 0:23:54.206805\n",
      "train loss 10.032619862741257\n",
      "val loss 10.393081842009554\n",
      "train loss 10.143884615142396\n",
      "val loss 10.10963421336816\n",
      "train loss 9.899545840647994\n",
      "val loss 9.955998776168883\n",
      "train loss 9.77765459130051\n",
      "val loss 9.961541058618241\n",
      "----End of step Time elapsed 0:23:54.061217\n",
      "train loss 9.829911367308812\n",
      "val loss 10.543932900364489\n",
      "train loss 10.063051333027111\n",
      "val loss 10.000012523309282\n",
      "train loss 9.809804759846534\n",
      "val loss 9.981474089112005\n",
      "train loss 9.585388116102079\n",
      "val loss 9.972602379879811\n",
      "----End of step Time elapsed 0:23:48.861925\n",
      "train loss 9.766636105190598\n",
      "val loss 9.945728419225997\n",
      "train loss 9.86276363927104\n",
      "val loss 9.948255405834418\n",
      "train loss 9.612947161600108\n",
      "val loss 9.942156813619253\n",
      "train loss 9.599326900618674\n",
      "val loss 9.89940027014591\n",
      "----End of step Time elapsed 0:23:54.735915\n",
      "train loss 9.657208188885278\n",
      "val loss 9.916111377380274\n",
      "train loss 9.86008643291079\n",
      "val loss 10.233702802544638\n",
      "train loss 9.557241638229723\n",
      "val loss 9.898299315541061\n"
     ]
    }
   ],
   "source": [
    "training_loop(model2, train_dl, valid_dl, steps=10, lr_low=1e-5, lr_high=0.001,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(model2, steps=3, lr_low=1e-5, lr_high=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PATH/\"mode1045.pth\"\n",
    "save_model(model, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PATH/\"mode1045.pth\"\n",
    "model.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(model2, steps=3, lr_low=1e-5, lr_high=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(model2, steps=3, lr_low=1e-5, lr_high=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss(model, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "model.load_state_dict(torch.load(p))\n",
    "p = PATH/\"model001.pth\"\n",
    "save_model(model, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* https://www.16bit.ai/blog/ml-and-future-of-radiology\n",
    "* https://stanfordmedicine.app.box.com/s/vhq1zop1867gr9rwnan4byj8lfxue173\n",
    "* https://github.com/fastai/fastai/blob/master/courses/dl2/cifar10-darknet.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want a beter model model?\n",
    "In this paper a better model is proposed.\n",
    "https://openreview.net/pdf?id=S1Ja7-2jf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
