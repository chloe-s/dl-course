{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classification with LSTM\n",
    "In this notebook we will use LSTMs to do sentiment classification on the [imdb dataset](http://ai.stanford.edu/~amaas/data/sentiment/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from spacy.symbols import ORTH\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data: <br>\n",
    "`wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/data2/yinterian/aclImdb/README'),\n",
       " PosixPath('/data2/yinterian/aclImdb/test'),\n",
       " PosixPath('/data2/yinterian/aclImdb/imdbEr.txt'),\n",
       " PosixPath('/data2/yinterian/aclImdb/train'),\n",
       " PosixPath('/data2/yinterian/aclImdb/imdb.vocab')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path(\"/data2/yinterian/aclImdb/\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = PATH/\"train/pos/0_9.txt\"\n",
    "path.read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first time run this\n",
    "#!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\n",
    "def sub_br(x): return re_br.sub(\"\\n\", x)\n",
    "\n",
    "my_tok = spacy.load('en')\n",
    "def spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(sub_br(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = PATH/\"train/pos/0_9.txt\"\n",
    "spacy_tok(path.read_text())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing vocab2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/data2/yinterian/aclImdb/train/pos/8030_9.txt'),\n",
       " PosixPath('/data2/yinterian/aclImdb/train/pos/8819_10.txt'),\n",
       " PosixPath('/data2/yinterian/aclImdb/train/pos/6316_8.txt'),\n",
       " PosixPath('/data2/yinterian/aclImdb/train/pos/4781_8.txt'),\n",
       " PosixPath('/data2/yinterian/aclImdb/train/pos/10085_10.txt')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_files = list((PATH/\"train\"/\"pos\").iterdir())\n",
    "neg_files = list((PATH/\"train\"/\"neg\").iterdir())\n",
    "all_files = pos_files + neg_files\n",
    "all_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "for path in all_files:\n",
    "    counts.update(spacy_tok(path.read_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103578"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in list(counts):\n",
    "    if counts[word] < 5:\n",
    "        del counts[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33918"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(path, vocab2index, N=120):\n",
    "    x = spacy_tok(path.read_text())\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in x])\n",
    "    l = min(N, len(enc1))\n",
    "    enc[N-l:] = enc1[:l]\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1,   774,   101,  2247,   101,   239,    22,  3051,   106,\n",
       "         455,   834,   123,    52,   940,   131,  1999,   276,  3050,\n",
       "        1040,    94,   416,  4813,    94,  4814,    76,  2336,  1100,\n",
       "          76, 31038,    47,   510,   145,  1661,    22,     1,    33,\n",
       "          25, 18194,   376,   746,   931,    74,  1480,   205,  2770,\n",
       "        3235,    52,     3,   392,  4605,    52, 11851,    29,  2879,\n",
       "          12,   276,    99,    25,  1580,  1190,    62,     8,    67,\n",
       "        6907,  2338,    47,   376,    58,    22,  2247,   376,  8076,\n",
       "       28445,    74,  1108,   793,  1436,   145,   302,    62,  1999,\n",
       "        1018,    47,   737,    74,    52,  1131,   847,  5916,    47,\n",
       "        2090,    74,   283,    63,    72,    52,  6027,  4495,     3,\n",
       "       18684,    74,   176,   518, 31038,    64, 14484,  8440,    47,\n",
       "          62,    67,  2748,  4313,    58,     5,    74, 29624,   171,\n",
       "         566,   176,   108,     1,   647,  4771,    72,    67,   166,\n",
       "         185,   145,  2295,    87,    74,  3662,  1342,   101,     5,\n",
       "         101,  1999,  3993,   300,    25,   825,   108, 13693,   392,\n",
       "          84,   277,  2979,    22,  1246,  2548,   836,    33,   442,\n",
       "        3559,  1064,    74, 12230,  1296,    74,   190,   703,   793,\n",
       "        1436,   145,   302,    22,  3051,   106,  3436, 28078,  8855,\n",
       "          47,  2315,    74,    90,    52,   278, 16405,    71,    72,\n",
       "         366,    25,  1580,     3,   116,    18,    52,  1034,   166,\n",
       "        3381,  4813,  4814,    74,   283,  3178,   227,   376,    33,\n",
       "         204,   376,  1839,    47,    84,   227,  7643,    29,   608,\n",
       "         793, 26013,   810,    29,   153,    42,   145,    25,  1818,\n",
       "          72,    52,     3,   354,    94, 31546,   235,    25,   136,\n",
       "        7360,  1580,  4829,    74,   123,  8790,   268,    25,  1580,\n",
       "          47,    54,     1,    22,  6014,  5823,     8,   376,  1370,\n",
       "         131,   271,    13,    22,  1511,  4194,  4829,   376,    25,\n",
       "       14484,  2548,  3760,    76,    22,  1414,   118,  2312,   451,\n",
       "          47,    25, 10699, 18194,  2059,    74,    91,    54,    99,\n",
       "        4813,  4814,   823,   108,   583,     8,    74,    10,    15,\n",
       "          54,   793,   410,   248,   116,  2247,   968,    24,   376,\n",
       "         831,    15,    74,  3662, 26163,    85,    62,   639,    22,\n",
       "       10959,  3449,   566,   271,   108,   442,  1264,  2165,   106,\n",
       "       12225,    24,    62,    74], dtype=int32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = PATH/\"train/neg/211_4.txt\"\n",
    "encode_sentence(path, vocab2index, N=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, PATH, train=\"train\", N=120):\n",
    "        self.N = N\n",
    "        self.path_to_images = PATH/train\n",
    "        self.pos_files = list((self.path_to_images/\"pos\").iterdir())\n",
    "        self.neg_files = list((self.path_to_images/\"neg\").iterdir())\n",
    "        self.files = self.pos_files + self.neg_files\n",
    "        # pos 1, neg 0\n",
    "        self.y = np.concatenate((np.ones(len(self.pos_files), dtype=int),\n",
    "                                np.zeros(len(self.neg_files), dtype=int)), axis=0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        return encode_sentence(path, vocab2index, self.N), self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImdbDataset(PATH)\n",
    "test_ds = ImdbDataset(PATH, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  293,  4435,   181,    76,   147,    52,    57,  1734,   132,\n",
       "          238,   137,   523,   463,  7319,    25,  3047,   180,  1879,\n",
       "         2744,  1694,    74,  4420,    52,  1261,    22, 14488, 31950,\n",
       "        16989,    47, 14488,     3,   185,  2019,  1331,    76,  1087,\n",
       "         5949,   250,    74,    90,   283,  6908,  1328, 14488,   123,\n",
       "         3408,     1,     1,    76,     1,   108,  3073,   755,   134,\n",
       "        16794,   131,     3,   577,  3040,    47,   392,  1087,   145,\n",
       "           51,    74,   608,   227,   978,   118,   333,    47,   342,\n",
       "         4439,    79,   140, 15342,    74,   283,   611,   147,    52,\n",
       "         1258,    25,   470,     3, 23727,    47,   777,  3608,    64,\n",
       "           25,   963, 24554,    47,   777,    40,  8328, 11888,   411,\n",
       "           25,   963,    24,   442,   870,  3418,   123,    52,   252,\n",
       "          166,  5810,    25,  2712,   106,   508, 10875,   392,    58,\n",
       "           70,   145,  5928], dtype=int32), 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(2, 4)  # Input dim is 2, output dim is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7992,  0.7052]],\n",
       "\n",
       "        [[-1.0743, -0.4082]],\n",
       "\n",
       "        [[-1.0387, -0.1603]],\n",
       "\n",
       "        [[-0.0198,  1.4817]],\n",
       "\n",
       "        [[-1.9672,  1.0904]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [torch.randn(1, 2) for _ in range(5)] # make a sequence of length 5\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = (torch.zeros(1, 1, 4), torch.zeros(1, 1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = lstm(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0737, -0.0569,  0.0293,  0.0154]],\n",
       "\n",
       "        [[ 0.0243, -0.1587,  0.0312, -0.1934]],\n",
       "\n",
       "        [[ 0.0166, -0.2160,  0.0273, -0.2424]],\n",
       "\n",
       "        [[ 0.1015, -0.2339,  0.0241, -0.0654]],\n",
       "\n",
       "        [[ 0.0615, -0.3624,  0.0121, -0.1566]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0615, -0.3624,  0.0121, -0.1566]]]),\n",
       " tensor([[[ 0.2976, -1.5159,  0.0268, -0.3277]]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 7\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 120])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(words)\n",
    "embedding_dim = 10\n",
    "embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 120, 10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = embed(x.long())\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 7, 10])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## x should have dimensions seq_len, batch_size, embedding dimension\n",
    "x = x.transpose(0,1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 9\n",
    "lstm = nn.LSTM(embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = (torch.zeros(1, batch_size, hidden_dim),\n",
    "     torch.zeros(1, batch_size, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = lstm(x, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 7, 9])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 9])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1, h2 = hidden\n",
    "h1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size) :\n",
    "        super(LSTMModel,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.linearOut = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, inputs, hidden) :\n",
    "        x = self.embeddings(inputs)\n",
    "        x = x.transpose(0,1)\n",
    "        lstm_out, lstm_h = self.lstm(x, hidden)\n",
    "        x = lstm_out[-1]\n",
    "        x = self.linearOut(x)\n",
    "        return x, lstm_h\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, self.batch_size, self.hidden_dim).cuda(),\n",
    "                torch.zeros(1, self.batch_size, self.hidden_dim).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y in train_dl:\n",
    "            x = x.long().cuda()\n",
    "            y = y.float().cuda()\n",
    "            hidden = model.init_hidden()\n",
    "            y_pred, _ = model(x, hidden)\n",
    "            model.zero_grad()\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        print(\"train loss %.3f\" % (sum_loss/total))\n",
    "        test_metrics(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metrics(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for x, y in test_dl:\n",
    "        x = x.long().cuda()\n",
    "        y = y.float().cuda().unsqueeze(1)\n",
    "        hidden = model.init_hidden()\n",
    "        y_hat, _ = model(x, hidden)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        y_pred = y_hat > 0\n",
    "        correct += (y_pred.float() == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "    print(\"test loss %.3f and accuracy %.3f\" % (sum_loss/total, correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImdbDataset(PATH, \"train\", 400)\n",
    "test_ds = ImdbDataset(PATH, \"test\", 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33920\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "model = LSTMModel(vocab_size, 50, 100, batch_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.696\n",
      "test loss 0.687 and accuracy 0.532\n",
      "train loss 0.680\n",
      "test loss 0.663 and accuracy 0.598\n",
      "train loss 0.728\n",
      "test loss 0.668 and accuracy 0.598\n",
      "train loss 0.669\n",
      "test loss 0.678 and accuracy 0.581\n",
      "train loss 0.668\n",
      "test loss 0.668 and accuracy 0.591\n",
      "train loss 0.643\n",
      "test loss 0.646 and accuracy 0.619\n",
      "train loss 0.589\n",
      "test loss 0.615 and accuracy 0.677\n",
      "train loss 0.576\n",
      "test loss 0.591 and accuracy 0.688\n",
      "train loss 0.534\n",
      "test loss 0.612 and accuracy 0.658\n",
      "train loss 0.516\n",
      "test loss 0.608 and accuracy 0.672\n",
      "train loss 0.480\n",
      "test loss 0.594 and accuracy 0.694\n",
      "train loss 0.421\n",
      "test loss 0.570 and accuracy 0.728\n",
      "train loss 0.367\n",
      "test loss 0.615 and accuracy 0.734\n",
      "train loss 0.351\n",
      "test loss 0.564 and accuracy 0.733\n",
      "train loss 0.317\n",
      "test loss 0.570 and accuracy 0.744\n",
      "train loss 0.279\n",
      "test loss 0.553 and accuracy 0.767\n",
      "train loss 0.236\n",
      "test loss 0.549 and accuracy 0.784\n",
      "train loss 0.215\n",
      "test loss 0.605 and accuracy 0.762\n",
      "train loss 0.226\n",
      "test loss 0.580 and accuracy 0.764\n",
      "train loss 0.189\n"
     ]
    }
   ],
   "source": [
    "model = train_epocs(model, epochs=20, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fighting overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model in this notebook is adapted from this [pytorch tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
