{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a language model with RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The data for this notebook can be downloaded from here\n",
    "`https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/data/yinterian/wikitext-2/valid.txt'),\n",
       " PosixPath('/data/yinterian/wikitext-2/train.txt'),\n",
       " PosixPath('/data/yinterian/wikitext-2/test.txt')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=Path(\"/data/yinterian/wikitext-2\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 44\n",
       " 45\n",
       " 46\n",
       " 47\n",
       " 48\n",
       " 49\n",
       " 45\n",
       " 50\n",
       " 51\n",
       " 42\n",
       "[torch.LongTensor of size 10]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = Corpus(PATH)\n",
    "corpus.test[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, batch_size)\n",
    "test_data = batchify(corpus.test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0     0    69  ...    973     0  1058\n",
       "    0     8    63  ...    204   281     0\n",
       "    0    85   149  ...    974   981   219\n",
       "       ...          â‹±          ...       \n",
       "   84   147   241  ...     45  1029  1105\n",
       "   21    63   242  ...   1009  1056     0\n",
       "    0   148   243  ...   1010  1057     0\n",
       "[torch.cuda.LongTensor of size 160x20 (GPU 0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Based on the model [here](https://github.com/pytorch/examples/tree/master/word_language_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        #self.decoder.bias.data.zero_()\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, bptt):\n",
    "        data, targets = get_batch(data_source, i, bptt, evaluation=True)\n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = Variable(hidden.data) #.detach()\n",
    "    return total_loss[0] / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    return Variable(h.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "bptt = 35\n",
    "clip = 0.25\n",
    "log_interval = 200 \n",
    "\n",
    "def get_batch(source, i, bptt, evaluation=False):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    return data, target\n",
    "    \n",
    "def train(model, lr):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i, bptt)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = Variable(hidden.data) #.detach()\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss[0] / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt, lr,\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.06s | valid loss  8.21 | valid ppl  3679.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss None\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.06s | valid loss  8.88 | valid ppl  7210.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.06s | valid loss  5.69 | valid ppl   296.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 8.210468292236328\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.06s | valid loss  5.59 | valid ppl   268.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 5.6930686950683596\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  0.05s | valid loss  5.46 | valid ppl   235.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 5.594561767578125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  0.05s | valid loss  5.65 | valid ppl   283.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  0.04s | valid loss  5.26 | valid ppl   192.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 5.463013076782227\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  0.04s | valid loss  5.18 | valid ppl   177.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 5.259437179565429\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  0.04s | valid loss  5.11 | valid ppl   165.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 5.178124237060547\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  0.04s | valid loss  5.04 | valid ppl   154.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 5.1067241668701175\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time:  0.04s | valid loss  4.98 | valid ppl   145.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 5.04002571105957\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time:  0.04s | valid loss  4.92 | valid ppl   136.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.980234527587891\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time:  0.04s | valid loss  4.86 | valid ppl   128.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.917575454711914\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time:  0.04s | valid loss  4.80 | valid ppl   121.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.858562850952149\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time:  0.04s | valid loss  4.74 | valid ppl   114.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.803108215332031\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time:  0.05s | valid loss  4.69 | valid ppl   108.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.742488861083984\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time:  0.05s | valid loss  4.63 | valid ppl   102.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.686215591430664\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time:  0.05s | valid loss  4.58 | valid ppl    97.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.631470489501953\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time:  0.05s | valid loss  4.54 | valid ppl    94.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.579557037353515\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time:  0.05s | valid loss  4.48 | valid ppl    88.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.543544006347656\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time:  0.04s | valid loss  4.43 | valid ppl    84.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.4846031188964846\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time:  0.04s | valid loss  4.39 | valid ppl    80.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.433555984497071\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time:  0.04s | valid loss  4.35 | valid ppl    77.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.388907241821289\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time:  0.04s | valid loss  4.29 | valid ppl    73.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.345757293701172\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time:  0.05s | valid loss  4.27 | valid ppl    71.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.293944549560547\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time:  0.05s | valid loss  4.22 | valid ppl    68.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.268378067016601\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time:  0.05s | valid loss  4.19 | valid ppl    66.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.224452209472656\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time:  0.05s | valid loss  4.15 | valid ppl    63.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.190033721923828\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time:  0.05s | valid loss  4.10 | valid ppl    60.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.151569747924805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time:  0.05s | valid loss  4.07 | valid ppl    58.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.1049541473388675\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time:  0.04s | valid loss  4.04 | valid ppl    56.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.072663879394531\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time:  0.04s | valid loss  4.00 | valid ppl    54.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.041250228881836\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time:  0.04s | valid loss  3.99 | valid ppl    54.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 4.004709243774414\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time:  0.04s | valid loss  3.94 | valid ppl    51.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 3.989274597167969\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time:  0.05s | valid loss  3.93 | valid ppl    50.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 3.9420055389404296\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time:  0.05s | valid loss  3.88 | valid ppl    48.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 3.9261573791503905\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time:  0.05s | valid loss  3.85 | valid ppl    46.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 3.879940414428711\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time:  0.05s | valid loss  3.81 | valid ppl    45.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 3.8461067199707033\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time:  0.05s | valid loss  3.78 | valid ppl    43.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 3.811494827270508\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time:  0.05s | valid loss  3.75 | valid ppl    42.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "best val loss 3.781535339355469\n"
     ]
    }
   ],
   "source": [
    "lr = 20.0\n",
    "best_val_loss = None\n",
    "epochs = 40\n",
    "nemb = 200\n",
    "nhid = 200\n",
    "nlayers = 2\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(ntokens, nemb, nhid, nlayers).cuda()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, lr)\n",
    "    val_loss = evaluate(val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "        val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        #with open(args.save, 'wb') as f:\n",
    "        #torch.save(model, f)\n",
    "        print(\"best val loss\", best_val_loss)\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
