{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a language model with RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wikitext-2\n",
    "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.\n",
    "\n",
    "The data can be dowloaded here.\n",
    "`https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/data/yinterian/wikitext-2/wiki.train.tokens'),\n",
       " PosixPath('/data/yinterian/wikitext-2/wiki.valid.tokens'),\n",
       " PosixPath('/data/yinterian/wikitext-2/wiki.test.tokens')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=Path(\"/data/yinterian/wikitext-2\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'wiki.train.tokens'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'wiki.valid.tokens'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'wiki.test.tokens'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33278"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = Corpus(PATH)\n",
    "len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, batch_size)\n",
    "test_data = batchify(corpus.test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104431"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Based on the model [here](https://github.com/pytorch/examples/tree/master/word_language_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.encoder.weight.data.uniform_(-init_range, init_range)\n",
    "        self.decoder.bias.data.fill_(0.0)\n",
    "        self.decoder.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, bptt, evaluation=False):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 20]) torch.Size([700])\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(train_data, 0, 35)\n",
    "print(x.data.shape, y.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "      0\n",
       "    284\n",
       "  15178\n",
       "    280\n",
       "    348\n",
       "    128\n",
       "    289\n",
       "   9493\n",
       "     16\n",
       "      1\n",
       "     13\n",
       "      0\n",
       "   2701\n",
       "   1227\n",
       "   1563\n",
       "   4044\n",
       "    115\n",
       "   1352\n",
       "   1335\n",
       "     16\n",
       " [torch.cuda.LongTensor of size 20 (GPU 0)], Variable containing:\n",
       "  1\n",
       " [torch.cuda.LongTensor of size 1 (GPU 0)])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:], y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify2(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 20])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.LongTensor(list(range(2005)))\n",
    "d = batchify2(data, 20)\n",
    "d.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, bptt, evaluation=False):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 20])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batch(d, i=0, bptt=15)\n",
    "x.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))\n",
    "\n",
    "def LR_range_finder(model, train_data, lr_low=1e-3, lr_high=10, epochs=2):\n",
    "    losses = []\n",
    "    (train_data.size(0) - 1)//bptt + 1\n",
    "    iterations = epochs * ((train_data.size(0) - 1)//bptt + 1)\n",
    "    delta = (lr_high - lr_low)/(iterations-1)\n",
    "    losses = []\n",
    "    lrs = [lr_low + i*delta for i in range(iterations)]\n",
    "    model.train()\n",
    "    ind = 0\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    for i in range(epochs):\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "            lr = lrs[ind]\n",
    "            data, targets = get_batch(train_data, i, bptt)\n",
    "        \n",
    "            hidden = Variable(hidden.data) #.detach()\n",
    "            model.zero_grad()\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = criterion(output.view(-1, ntokens), targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "            losses.append(loss.data[0])\n",
    "            ind += 1\n",
    "    return lrs, losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemb = 300\n",
    "nhid = 300\n",
    "nlayers = 2\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(ntokens, nemb, nhid, nlayers).cuda()\n",
    "lrs, losses = LR_range_finder(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5968"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(lrs, losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triangular_lr2(lr_low, lr_high, iterations):\n",
    "    iter1 = int(0.35*iterations)\n",
    "    iter2 = int(0.85*iter1)\n",
    "    iter3 = iterations - iter1 - iter2\n",
    "    delta1 = (lr_high - lr_low)/iter1\n",
    "    delta2 = (lr_high - lr_low)/(iter1 -1)\n",
    "    lrs1 = [lr_low + i*delta1 for i in range(iter1)]\n",
    "    lrs2 = [lr_high - i*(delta1) for i in range(0, iter2)]\n",
    "    delta2 = (lrs2[-1] - lr_low)/(iter3)\n",
    "    lrs3 = [lrs2[-1] - i*(delta2) for i in range(1, iter3+1)]\n",
    "    return lrs1+lrs2+lrs3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, bptt):\n",
    "        data, targets = get_batch(data_source, i, bptt, evaluation=True)\n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = Variable(hidden.data) #.detach()\n",
    "    return total_loss[0] / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    return Variable(h.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "bptt = 35\n",
    "clip = 0.25\n",
    "\n",
    "def get_batch(source, i, bptt, evaluation=False):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    return data, target\n",
    "    \n",
    "def train_triangular_policy(model, epochs=4, lr_low=1e-4, lr_high=4):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    iterations = epochs * ((train_data.size(0) - 1)//bptt + 1)\n",
    "    lrs = get_triangular_lr2(lr_low, lr_high, iterations)\n",
    "    idx = 0\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "            lr = lrs[idx]\n",
    "            data, targets = get_batch(train_data, i, bptt)\n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            hidden = Variable(hidden.data) #.detach()\n",
    "            model.zero_grad()\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = criterion(output.view(-1, ntokens), targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "            total_loss += len(data)*loss.data\n",
    "            idx += 1\n",
    "        # results after each epoch\n",
    "        val_loss = evaluate(val_data)\n",
    "        elapsed = time.time() - start_time\n",
    "        train_loss = total_loss[0]/len(train_data)\n",
    "        print('| epoch {:3d} | lr {:02.5f} | t_loss {:5.2f} | t_ppl {:5.2f} | v_loss {:5.2f} | v_ppl {:5.2f}'.format(\n",
    "             epoch, lr, train_loss, math.exp(train_loss), val_loss, math.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10\n",
    "nemb = 300\n",
    "nhid = 300\n",
    "nlayers = 2\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(ntokens, nemb, nhid, nlayers).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 8.28489 | t_loss  6.51 | t_ppl 674.11 | v_loss  5.87 | v_ppl 354.31\n",
      "| epoch   1 | lr 7.42878 | t_loss  5.81 | t_ppl 333.00 | v_loss  5.47 | v_ppl 237.30\n",
      "| epoch   2 | lr 4.63954 | t_loss  5.47 | t_ppl 237.08 | v_loss  5.28 | v_ppl 196.77\n",
      "| epoch   3 | lr 4.00000 | t_loss  5.30 | t_ppl 200.27 | v_loss  5.20 | v_ppl 180.44\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=4, lr_high=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 8.28489 | t_loss  5.23 | t_ppl 185.87 | v_loss  5.27 | v_ppl 195.36\n",
      "| epoch   1 | lr 7.42878 | t_loss  5.21 | t_ppl 182.71 | v_loss  5.14 | v_ppl 170.18\n",
      "| epoch   2 | lr 4.63954 | t_loss  5.05 | t_ppl 155.47 | v_loss  5.05 | v_ppl 155.32\n",
      "| epoch   3 | lr 4.00000 | t_loss  4.95 | t_ppl 140.87 | v_loss  5.02 | v_ppl 150.83\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=4, lr_high=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 6.28489 | t_loss  4.89 | t_ppl 132.78 | v_loss  5.05 | v_ppl 155.79\n",
      "| epoch   1 | lr 5.42878 | t_loss  4.92 | t_ppl 136.97 | v_loss  5.00 | v_ppl 148.77\n",
      "| epoch   2 | lr 2.63954 | t_loss  4.81 | t_ppl 122.94 | v_loss  4.93 | v_ppl 138.66\n",
      "| epoch   3 | lr 2.00000 | t_loss  4.74 | t_ppl 114.89 | v_loss  4.91 | v_ppl 135.16\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=2, lr_high=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 4.57074 | t_loss  4.72 | t_ppl 111.93 | v_loss  4.94 | v_ppl 139.20\n",
      "| epoch   1 | lr 3.85731 | t_loss  4.75 | t_ppl 115.95 | v_loss  4.92 | v_ppl 136.52\n",
      "| epoch   2 | lr 1.53295 | t_loss  4.68 | t_ppl 107.79 | v_loss  4.87 | v_ppl 130.26\n",
      "| epoch   3 | lr 1.00000 | t_loss  4.64 | t_ppl 103.23 | v_loss  4.86 | v_ppl 128.61\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=1, lr_high=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 4.57074 | t_loss  4.63 | t_ppl 102.67 | v_loss  4.92 | v_ppl 137.39\n",
      "| epoch   1 | lr 3.85731 | t_loss  4.68 | t_ppl 107.87 | v_loss  4.90 | v_ppl 134.66\n",
      "| epoch   2 | lr 1.53295 | t_loss  4.61 | t_ppl 100.48 | v_loss  4.85 | v_ppl 128.21\n",
      "| epoch   3 | lr 1.00000 | t_loss  4.57 | t_ppl 96.35 | v_loss  4.84 | v_ppl 126.53\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=1, lr_high=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 4.57074 | t_loss  4.56 | t_ppl 96.05 | v_loss  4.89 | v_ppl 133.27\n",
      "| epoch   1 | lr 3.85731 | t_loss  4.62 | t_ppl 101.44 | v_loss  4.89 | v_ppl 132.65\n",
      "| epoch   2 | lr 1.53295 | t_loss  4.55 | t_ppl 94.80 | v_loss  4.84 | v_ppl 126.95\n",
      "| epoch   3 | lr 1.00000 | t_loss  4.51 | t_ppl 90.96 | v_loss  4.82 | v_ppl 124.18\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=1, lr_high=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 2.99952 | t_loss  4.50 | t_ppl 89.74 | v_loss  4.86 | v_ppl 129.32\n",
      "| epoch   1 | lr 2.50012 | t_loss  4.53 | t_ppl 92.43 | v_loss  4.84 | v_ppl 127.01\n",
      "| epoch   2 | lr 0.87306 | t_loss  4.49 | t_ppl 89.01 | v_loss  4.82 | v_ppl 123.92\n",
      "| epoch   3 | lr 0.50000 | t_loss  4.47 | t_ppl 87.05 | v_loss  4.81 | v_ppl 122.50\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=0.5, lr_high=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 2.85945 | t_loss  4.46 | t_ppl 86.56 | v_loss  4.85 | v_ppl 127.17\n",
      "| epoch   1 | lr 2.29014 | t_loss  4.49 | t_ppl 89.35 | v_loss  4.84 | v_ppl 126.69\n",
      "| epoch   2 | lr 0.43529 | t_loss  4.46 | t_ppl 86.33 | v_loss  4.80 | v_ppl 121.29\n",
      "| epoch   3 | lr 0.01000 | t_loss  4.47 | t_ppl 87.38 | v_loss  4.77 | v_ppl 118.18\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=0.01, lr_high=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 2.14531 | t_loss  4.43 | t_ppl 84.30 | v_loss  4.83 | v_ppl 125.45\n",
      "| epoch   1 | lr 1.71867 | t_loss  4.45 | t_ppl 85.35 | v_loss  4.82 | v_ppl 124.22\n",
      "| epoch   2 | lr 0.32870 | t_loss  4.43 | t_ppl 83.84 | v_loss  4.79 | v_ppl 120.38\n",
      "| epoch   3 | lr 0.01000 | t_loss  4.45 | t_ppl 85.78 | v_loss  4.77 | v_ppl 117.60\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=0.01, lr_high=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 1.42858 | t_loss  4.40 | t_ppl 81.53 | v_loss  4.81 | v_ppl 122.89\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=0.001, lr_high=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PATH/\"mode117.pth\"\n",
    "save_model(model, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
