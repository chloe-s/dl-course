{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjectivity classification using a pretrained language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we implement the approched described in cite... Jeremy's paper In particular, we will classify sentences into \"subjective\" or \"objective\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjectivity Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subjectivity dataset has 5000 subjective and 5000 objective processed sentences. To get the data:\n",
    "```\n",
    "wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/data/yinterian/rotten_imdb/subjdata.README.1.0'),\n",
       " PosixPath('/data/yinterian/rotten_imdb/plot.tok.gt9.5000'),\n",
       " PosixPath('/data/yinterian/rotten_imdb/quote.tok.gt9.5000')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path(\"/data/yinterian/rotten_imdb/\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the readme file:\n",
    "- quote.tok.gt9.5000 contains 5000 subjective sentences (or snippets)\n",
    "- plot.tok.gt9.5000 contains 5000 objective sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the movie begins in the past where a young boy named sam attempts to save celebi from a hunter . \r\n",
      "emerging from the human psyche and showing characteristics of abstract expressionism , minimalism and russian constructivism , graffiti removal has secured its place in the history of modern art while being created by artists who are unconscious of their artistic achievements . \r\n",
      "spurning her mother's insistence that she get on with her life , mary is thrown out of the house , rejected by joe , and expelled from school as she grows larger with child . \r\n",
      "amitabh can't believe the board of directors and his mind is filled with revenge and what better revenge than robbing the bank himself , ironic as it may sound . \r\n",
      "she , among others excentricities , talks to a small rock , gertrude , like if she was alive . \r\n",
      "this gives the girls a fair chance of pulling the wool over their eyes using their sexiness to poach any last vestige of common sense the dons might have had . \r\n",
      "styled after vh1's \" behind the music , \" this mockumentary profiles the rise and fall of an internet startup , called icevan . com . \r\n",
      "being blue is not his only predicament ; he also lacks the ability to outwardly express his emotions . \r\n",
      "the killer's clues are a perversion of biblical punishments for sins : stoning , burning , decapitation . \r\n",
      "david is a painter with painter's block who takes a job as a waiter to get some inspiration . \r\n"
     ]
    }
   ],
   "source": [
    "! head /data/yinterian/rotten_imdb/plot.tok.gt9.5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# this is from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \"\"\" Read file returns a shuttled list.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = np.array(f.readlines())\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "def get_vocab(list_of_content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for content in list_of_content:\n",
    "        for line in content:\n",
    "            line = clean_str(line.strip())\n",
    "            words = set(line.split())\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "    return vocab       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_content = read_file(PATH/\"quote.tok.gt9.5000\")\n",
    "obj_content = read_file(PATH/\"plot.tok.gt9.5000\")\n",
    "sub_content = np.array([clean_str(line.strip()) for line in sub_content])\n",
    "obj_content = np.array([clean_str(line.strip()) for line in obj_content])\n",
    "sub_y = np.zeros(len(sub_content))\n",
    "obj_y = np.ones(len(obj_content))\n",
    "X = np.append(sub_content, obj_content)\n",
    "y = np.append(sub_y, obj_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['will god let her fall or give her a new path \\\\?',\n",
       "        \"the director 's twitchy sketchbook style and adroit perspective shifts grow wearisome amid leaden pacing and indifferent craftsmanship \\\\( most notably wretched sound design \\\\)\",\n",
       "        \"welles groupie scholar peter bogdanovich took a long time to do it , but he 's finally provided his own broadside at publishing giant william randolph hearst\",\n",
       "        'based on the 1997 john king novel of the same name with a rather odd synopsis a first novel about a seasoned chelsea football club hooligan who represents a disaffected society operating by brutal rules',\n",
       "        'yet , beneath an upbeat appearance , she is struggling desperately with the emotional and physical scars left by the attack'],\n",
       "       dtype='<U679'), array([1., 0., 0., 1., 1.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting vocab from training sets\n",
    "vocab = get_vocab([X_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laguage Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.encoder.weight.data.uniform_(-init_range, init_range)\n",
    "        self.decoder.bias.data.fill_(0.0)\n",
    "        self.decoder.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemb = 300\n",
    "nhid = 300\n",
    "nlayers = 2\n",
    "ntokens = 33278\n",
    "lang_model = RNNModel(ntokens, nemb, nhid, nlayers).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load pre-trained model\n",
    "model_path = \"/data/yinterian/wikitext-2/mode117.pth\"\n",
    "def load_model(m, p):\n",
    "    m.load_state_dict(torch.load(p))\n",
    "load_model(lang_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel (\n",
       "  (drop): Dropout (p = 0.5)\n",
       "  (encoder): Embedding(33278, 300)\n",
       "  (rnn): GRU(300, 300, num_layers=2, dropout=0.5)\n",
       "  (decoder): Linear (300 -> 33278)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding data with original corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path, train_file, valid_file, test_file=None, word2idx=None):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.word2idx = word2idx\n",
    "        if word2idx is None:\n",
    "            tr_tokens = self.add_words(os.path.join(path, train_file))\n",
    "            val_tokens = self.add_words(os.path.join(path, valid_file))\n",
    "            test_tokens = self.add_words(os.path.join(path, test_file))\n",
    "            self.word2idx = self.dictionary.word2idx\n",
    "        self.train = self.tokenize(os.path.join(path, train_file), tr_tokens)\n",
    "        self.valid = self.tokenize(os.path.join(path, valid_file), val_tokens)\n",
    "        self.test = self.tokenize(os.path.join(path, test_file), test_tokens)\n",
    "        \n",
    "    def add_words(self, path):\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "        return tokens\n",
    "\n",
    "    def tokenize(self, path, tokens):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.word2idx[word]\n",
    "                    token += 1\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_corpus = Corpus(\"/data/yinterian/wikitext-2\",\"wiki.train.tokens\", \"wiki.valid.tokens\", \"wiki.test.tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<eos>': 0,\n",
       " '=': 1,\n",
       " 'Valkyria': 2,\n",
       " 'Chronicles': 3,\n",
       " 'III': 4,\n",
       " 'Senjō': 5,\n",
       " 'no': 6,\n",
       " '3': 7,\n",
       " ':': 8,\n",
       " '<unk>': 9,\n",
       " '(': 10,\n",
       " 'Japanese': 11,\n",
       " '戦場のヴァルキュリア3': 12,\n",
       " ',': 13,\n",
       " 'lit': 14,\n",
       " '.': 15,\n",
       " 'of': 16,\n",
       " 'the': 17,\n",
       " 'Battlefield': 18,\n",
       " ')': 19,\n",
       " 'commonly': 20,\n",
       " 'referred': 21,\n",
       " 'to': 22,\n",
       " 'as': 23,\n",
       " 'outside': 24,\n",
       " 'Japan': 25,\n",
       " 'is': 26,\n",
       " 'a': 27,\n",
       " 'tactical': 28,\n",
       " 'role': 29,\n",
       " '@-@': 30,\n",
       " 'playing': 31,\n",
       " 'video': 32,\n",
       " 'game': 33,\n",
       " 'developed': 34,\n",
       " 'by': 35,\n",
       " 'Sega': 36,\n",
       " 'and': 37,\n",
       " 'Media.Vision': 38,\n",
       " 'for': 39,\n",
       " 'PlayStation': 40,\n",
       " 'Portable': 41,\n",
       " 'Released': 42,\n",
       " 'in': 43,\n",
       " 'January': 44,\n",
       " '2011': 45,\n",
       " 'it': 46,\n",
       " 'third': 47,\n",
       " 'series': 48,\n",
       " 'same': 49,\n",
       " 'fusion': 50,\n",
       " 'real': 51,\n",
       " 'time': 52,\n",
       " 'gameplay': 53,\n",
       " 'its': 54,\n",
       " 'predecessors': 55,\n",
       " 'story': 56,\n",
       " 'runs': 57,\n",
       " 'parallel': 58,\n",
       " 'first': 59,\n",
       " 'follows': 60,\n",
       " '\"': 61,\n",
       " 'Nameless': 62,\n",
       " 'penal': 63,\n",
       " 'military': 64,\n",
       " 'unit': 65,\n",
       " 'serving': 66,\n",
       " 'nation': 67,\n",
       " 'Gallia': 68,\n",
       " 'during': 69,\n",
       " 'Second': 70,\n",
       " 'Europan': 71,\n",
       " 'War': 72,\n",
       " 'who': 73,\n",
       " 'perform': 74,\n",
       " 'secret': 75,\n",
       " 'black': 76,\n",
       " 'operations': 77,\n",
       " 'are': 78,\n",
       " 'pitted': 79,\n",
       " 'against': 80,\n",
       " 'Imperial': 81,\n",
       " 'Raven': 82,\n",
       " 'The': 83,\n",
       " 'began': 84,\n",
       " 'development': 85,\n",
       " '2010': 86,\n",
       " 'carrying': 87,\n",
       " 'over': 88,\n",
       " 'large': 89,\n",
       " 'portion': 90,\n",
       " 'work': 91,\n",
       " 'done': 92,\n",
       " 'on': 93,\n",
       " 'II': 94,\n",
       " 'While': 95,\n",
       " 'retained': 96,\n",
       " 'standard': 97,\n",
       " 'features': 98,\n",
       " 'also': 99,\n",
       " 'underwent': 100,\n",
       " 'multiple': 101,\n",
       " 'adjustments': 102,\n",
       " 'such': 103,\n",
       " 'making': 104,\n",
       " 'more': 105,\n",
       " 'newcomers': 106,\n",
       " 'Character': 107,\n",
       " 'designer': 108,\n",
       " 'Honjou': 109,\n",
       " 'composer': 110,\n",
       " 'Hitoshi': 111,\n",
       " 'Sakimoto': 112,\n",
       " 'both': 113,\n",
       " 'returned': 114,\n",
       " 'from': 115,\n",
       " 'previous': 116,\n",
       " 'entries': 117,\n",
       " 'along': 118,\n",
       " 'with': 119,\n",
       " 'director': 120,\n",
       " 'Takeshi': 121,\n",
       " 'Ozawa': 122,\n",
       " 'A': 123,\n",
       " 'team': 124,\n",
       " 'writers': 125,\n",
       " 'handled': 126,\n",
       " 'script': 127,\n",
       " \"'s\": 128,\n",
       " 'opening': 129,\n",
       " 'theme': 130,\n",
       " 'was': 131,\n",
       " 'sung': 132,\n",
       " 'May': 133,\n",
       " \"'n\": 134,\n",
       " 'It': 135,\n",
       " 'met': 136,\n",
       " 'positive': 137,\n",
       " 'sales': 138,\n",
       " 'praised': 139,\n",
       " 'western': 140,\n",
       " 'critics': 141,\n",
       " 'After': 142,\n",
       " 'release': 143,\n",
       " 'received': 144,\n",
       " 'downloadable': 145,\n",
       " 'content': 146,\n",
       " 'an': 147,\n",
       " 'expanded': 148,\n",
       " 'edition': 149,\n",
       " 'November': 150,\n",
       " 'that': 151,\n",
       " 'year': 152,\n",
       " 'adapted': 153,\n",
       " 'into': 154,\n",
       " 'manga': 155,\n",
       " 'original': 156,\n",
       " 'animation': 157,\n",
       " 'Due': 158,\n",
       " 'low': 159,\n",
       " 'not': 160,\n",
       " 'localized': 161,\n",
       " 'but': 162,\n",
       " 'fan': 163,\n",
       " 'translation': 164,\n",
       " 'compatible': 165,\n",
       " 'released': 166,\n",
       " '2014': 167,\n",
       " 'would': 168,\n",
       " 'return': 169,\n",
       " 'franchise': 170,\n",
       " 'Azure': 171,\n",
       " 'Revolution': 172,\n",
       " '4': 173,\n",
       " 'Gameplay': 174,\n",
       " 'As': 175,\n",
       " 'games': 176,\n",
       " 'where': 177,\n",
       " 'players': 178,\n",
       " 'take': 179,\n",
       " 'control': 180,\n",
       " 'part': 181,\n",
       " 'missions': 182,\n",
       " 'enemy': 183,\n",
       " 'forces': 184,\n",
       " 'Stories': 185,\n",
       " 'told': 186,\n",
       " 'through': 187,\n",
       " 'comic': 188,\n",
       " 'book': 189,\n",
       " 'like': 190,\n",
       " 'panels': 191,\n",
       " 'animated': 192,\n",
       " 'character': 193,\n",
       " 'portraits': 194,\n",
       " 'characters': 195,\n",
       " 'speaking': 196,\n",
       " 'partially': 197,\n",
       " 'voiced': 198,\n",
       " 'speech': 199,\n",
       " 'bubbles': 200,\n",
       " 'text': 201,\n",
       " 'player': 202,\n",
       " 'progresses': 203,\n",
       " 'linear': 204,\n",
       " 'gradually': 205,\n",
       " 'unlocked': 206,\n",
       " 'maps': 207,\n",
       " 'can': 208,\n",
       " 'be': 209,\n",
       " 'freely': 210,\n",
       " 'replayed': 211,\n",
       " 'they': 212,\n",
       " 'route': 213,\n",
       " 'each': 214,\n",
       " 'location': 215,\n",
       " 'map': 216,\n",
       " 'varies': 217,\n",
       " 'depending': 218,\n",
       " 'individual': 219,\n",
       " 'approach': 220,\n",
       " 'when': 221,\n",
       " 'one': 222,\n",
       " 'option': 223,\n",
       " 'selected': 224,\n",
       " 'other': 225,\n",
       " 'sealed': 226,\n",
       " 'off': 227,\n",
       " 'Outside': 228,\n",
       " 'rest': 229,\n",
       " 'camp': 230,\n",
       " 'units': 231,\n",
       " 'customized': 232,\n",
       " 'growth': 233,\n",
       " 'occurs': 234,\n",
       " 'Alongside': 235,\n",
       " 'main': 236,\n",
       " 'specific': 237,\n",
       " 'sub': 238,\n",
       " 'relating': 239,\n",
       " 'different': 240,\n",
       " 'squad': 241,\n",
       " 'members': 242,\n",
       " 'completion': 243,\n",
       " 'additional': 244,\n",
       " 'episodes': 245,\n",
       " 'some': 246,\n",
       " 'them': 247,\n",
       " 'having': 248,\n",
       " 'higher': 249,\n",
       " 'difficulty': 250,\n",
       " 'than': 251,\n",
       " 'those': 252,\n",
       " 'found': 253,\n",
       " 'There': 254,\n",
       " 'love': 255,\n",
       " 'simulation': 256,\n",
       " 'elements': 257,\n",
       " 'related': 258,\n",
       " 'two': 259,\n",
       " 'although': 260,\n",
       " 'very': 261,\n",
       " 'minor': 262,\n",
       " 'battle': 263,\n",
       " 'system': 264,\n",
       " 'carried': 265,\n",
       " 'directly': 266,\n",
       " 'During': 267,\n",
       " 'select': 268,\n",
       " 'using': 269,\n",
       " 'top': 270,\n",
       " 'down': 271,\n",
       " 'perspective': 272,\n",
       " 'battlefield': 273,\n",
       " 'once': 274,\n",
       " 'moves': 275,\n",
       " 'around': 276,\n",
       " 'person': 277,\n",
       " 'only': 278,\n",
       " 'act': 279,\n",
       " 'per': 280,\n",
       " 'turn': 281,\n",
       " 'granted': 282,\n",
       " 'turns': 283,\n",
       " 'at': 284,\n",
       " 'expense': 285,\n",
       " \"'\": 286,\n",
       " 'Each': 287,\n",
       " 'has': 288,\n",
       " 'field': 289,\n",
       " 'distance': 290,\n",
       " 'movement': 291,\n",
       " 'limited': 292,\n",
       " 'their': 293,\n",
       " 'Action': 294,\n",
       " 'Up': 295,\n",
       " 'nine': 296,\n",
       " 'assigned': 297,\n",
       " 'single': 298,\n",
       " 'mission': 299,\n",
       " 'will': 300,\n",
       " 'call': 301,\n",
       " 'out': 302,\n",
       " 'if': 303,\n",
       " 'something': 304,\n",
       " 'happens': 305,\n",
       " 'health': 306,\n",
       " 'points': 307,\n",
       " 'HP': 308,\n",
       " 'getting': 309,\n",
       " 'or': 310,\n",
       " 'being': 311,\n",
       " 'knocked': 312,\n",
       " 'attacks': 313,\n",
       " 'Potentials': 314,\n",
       " 'skills': 315,\n",
       " 'unique': 316,\n",
       " 'They': 317,\n",
       " 'divided': 318,\n",
       " 'Personal': 319,\n",
       " 'Potential': 320,\n",
       " 'which': 321,\n",
       " 'innate': 322,\n",
       " 'remain': 323,\n",
       " 'unaltered': 324,\n",
       " 'unless': 325,\n",
       " 'otherwise': 326,\n",
       " 'dictated': 327,\n",
       " 'either': 328,\n",
       " 'help': 329,\n",
       " 'impede': 330,\n",
       " 'Battle': 331,\n",
       " 'grown': 332,\n",
       " 'throughout': 333,\n",
       " 'always': 334,\n",
       " 'grant': 335,\n",
       " 'To': 336,\n",
       " 'learn': 337,\n",
       " 'Masters': 338,\n",
       " 'Table': 339,\n",
       " 'grid': 340,\n",
       " 'based': 341,\n",
       " 'skill': 342,\n",
       " 'table': 343,\n",
       " 'used': 344,\n",
       " 'acquire': 345,\n",
       " 'link': 346,\n",
       " 'Characters': 347,\n",
       " 'have': 348,\n",
       " 'Special': 349,\n",
       " 'temporary': 350,\n",
       " 'Kurt': 351,\n",
       " 'activate': 352,\n",
       " 'Direct': 353,\n",
       " 'Command': 354,\n",
       " 'move': 355,\n",
       " 'without': 356,\n",
       " 'his': 357,\n",
       " 'Point': 358,\n",
       " 'gauge': 359,\n",
       " 'shift': 360,\n",
       " 'her': 361,\n",
       " 'Form': 362,\n",
       " 'become': 363,\n",
       " 'while': 364,\n",
       " 'Imca': 365,\n",
       " 'target': 366,\n",
       " 'heavy': 367,\n",
       " 'weapon': 368,\n",
       " 'Troops': 369,\n",
       " 'five': 370,\n",
       " 'classes': 371,\n",
       " 'Scouts': 372,\n",
       " 'Engineers': 373,\n",
       " 'Armored': 374,\n",
       " 'Soldier': 375,\n",
       " 'switch': 376,\n",
       " 'changing': 377,\n",
       " 'Changing': 378,\n",
       " 'class': 379,\n",
       " 'does': 380,\n",
       " 'greatly': 381,\n",
       " 'affect': 382,\n",
       " 'stats': 383,\n",
       " 'gained': 384,\n",
       " 'With': 385,\n",
       " 'victory': 386,\n",
       " 'experience': 387,\n",
       " 'awarded': 388,\n",
       " 'distributed': 389,\n",
       " 'attributes': 390,\n",
       " 'shared': 391,\n",
       " 'entire': 392,\n",
       " 'feature': 393,\n",
       " 'differing': 394,\n",
       " 'early': 395,\n",
       " 'method': 396,\n",
       " 'distributing': 397,\n",
       " 'types': 398,\n",
       " 'Plot': 399,\n",
       " 'takes': 400,\n",
       " 'place': 401,\n",
       " 'Gallian': 402,\n",
       " 'Army': 403,\n",
       " 'Squad': 404,\n",
       " '422': 405,\n",
       " 'known': 406,\n",
       " 'composed': 407,\n",
       " 'criminals': 408,\n",
       " 'foreign': 409,\n",
       " 'offenders': 410,\n",
       " 'whose': 411,\n",
       " 'names': 412,\n",
       " 'erased': 413,\n",
       " 'records': 414,\n",
       " 'officially': 415,\n",
       " 'numbers': 416,\n",
       " 'most': 417,\n",
       " 'dangerous': 418,\n",
       " 'Regular': 419,\n",
       " 'Militia': 420,\n",
       " 'do': 421,\n",
       " 'nevertheless': 422,\n",
       " 'up': 423,\n",
       " 'task': 424,\n",
       " 'exemplified': 425,\n",
       " 'motto': 426,\n",
       " 'meaning': 427,\n",
       " 'Always': 428,\n",
       " 'Ready': 429,\n",
       " 'three': 430,\n",
       " 'Irving': 431,\n",
       " 'army': 432,\n",
       " 'officer': 433,\n",
       " 'falsely': 434,\n",
       " 'accused': 435,\n",
       " 'treason': 436,\n",
       " 'wishes': 437,\n",
       " 'redeem': 438,\n",
       " 'himself': 439,\n",
       " ';': 440,\n",
       " 'Ace': 441,\n",
       " 'female': 442,\n",
       " 'Darcsen': 443,\n",
       " 'weapons': 444,\n",
       " 'specialist': 445,\n",
       " 'seeks': 446,\n",
       " 'revenge': 447,\n",
       " 'destroyed': 448,\n",
       " 'home': 449,\n",
       " 'Riela': 450,\n",
       " 'seemingly': 451,\n",
       " 'young': 452,\n",
       " 'woman': 453,\n",
       " 'unknowingly': 454,\n",
       " 'descendant': 455,\n",
       " 'Together': 456,\n",
       " 'fellow': 457,\n",
       " 'these': 458,\n",
       " 'tasked': 459,\n",
       " 'fight': 460,\n",
       " 'mysterious': 461,\n",
       " 'Calamity': 462,\n",
       " 'consisting': 463,\n",
       " 'mostly': 464,\n",
       " 'soldiers': 465,\n",
       " 'exist': 466,\n",
       " 'upper': 467,\n",
       " 'echelons': 468,\n",
       " 'exploit': 469,\n",
       " 'concept': 470,\n",
       " 'plausible': 471,\n",
       " 'order': 472,\n",
       " 'send': 473,\n",
       " 'make': 474,\n",
       " 'lose': 475,\n",
       " 'face': 476,\n",
       " 'war': 477,\n",
       " 'times': 478,\n",
       " 'this': 479,\n",
       " 'works': 480,\n",
       " 'advantage': 481,\n",
       " 'successful': 482,\n",
       " 'incursion': 483,\n",
       " 'territory': 484,\n",
       " 'orders': 485,\n",
       " 'cause': 486,\n",
       " 'certain': 487,\n",
       " '422nd': 488,\n",
       " 'great': 489,\n",
       " 'distress': 490,\n",
       " 'One': 491,\n",
       " 'member': 492,\n",
       " 'becomes': 493,\n",
       " 'so': 494,\n",
       " 'enraged': 495,\n",
       " 'he': 496,\n",
       " 'abandons': 497,\n",
       " 'post': 498,\n",
       " 'defects': 499,\n",
       " 'ranks': 500,\n",
       " 'attached': 501,\n",
       " 'ideal': 502,\n",
       " 'independence': 503,\n",
       " 'proposed': 504,\n",
       " 'leader': 505,\n",
       " 'Dahau': 506,\n",
       " 'At': 507,\n",
       " 'within': 508,\n",
       " 'erase': 509,\n",
       " 'protect': 510,\n",
       " 'own': 511,\n",
       " 'interests': 512,\n",
       " 'allies': 513,\n",
       " 'enemies': 514,\n",
       " 'combined': 515,\n",
       " 'presence': 516,\n",
       " 'traitor': 517,\n",
       " 'desperately': 518,\n",
       " 'keep': 519,\n",
       " 'themselves': 520,\n",
       " 'alive': 521,\n",
       " 'effort': 522,\n",
       " 'This': 523,\n",
       " 'continues': 524,\n",
       " 'until': 525,\n",
       " 'commanding': 526,\n",
       " 'Ramsey': 527,\n",
       " 'Crowe': 528,\n",
       " 'had': 529,\n",
       " 'been': 530,\n",
       " 'kept': 531,\n",
       " 'under': 532,\n",
       " 'house': 533,\n",
       " 'arrest': 534,\n",
       " 'escorted': 535,\n",
       " 'capital': 536,\n",
       " 'city': 537,\n",
       " 'present': 538,\n",
       " 'evidence': 539,\n",
       " 'weary': 540,\n",
       " 'expose': 541,\n",
       " 'General': 542,\n",
       " 'Treason': 543,\n",
       " 'due': 544,\n",
       " 'events': 545,\n",
       " 'partly': 546,\n",
       " 'major': 547,\n",
       " 'losses': 548,\n",
       " 'manpower': 549,\n",
       " 'suffers': 550,\n",
       " 'towards': 551,\n",
       " 'end': 552,\n",
       " 'Empire': 553,\n",
       " 'offered': 554,\n",
       " 'formal': 555,\n",
       " 'position': 556,\n",
       " 'rather': 557,\n",
       " 'serve': 558,\n",
       " 'anonymous': 559,\n",
       " 'shadow': 560,\n",
       " 'force': 561,\n",
       " 'short': 562,\n",
       " 'lived': 563,\n",
       " 'however': 564,\n",
       " 'following': 565,\n",
       " 'Maximilian': 566,\n",
       " 'defeat': 567,\n",
       " 'ancient': 568,\n",
       " 'super': 569,\n",
       " 'benefactor': 570,\n",
       " 'Without': 571,\n",
       " 'support': 572,\n",
       " 'chance': 573,\n",
       " 'prove': 574,\n",
       " 'last': 575,\n",
       " 'card': 576,\n",
       " 'creating': 577,\n",
       " 'new': 578,\n",
       " 'armed': 579,\n",
       " 'invading': 580,\n",
       " 'just': 581,\n",
       " 'nations': 582,\n",
       " 'cease': 583,\n",
       " 'fire': 584,\n",
       " 'certainly': 585,\n",
       " 'wreck': 586,\n",
       " 'newfound': 587,\n",
       " 'peace': 588,\n",
       " 'decides': 589,\n",
       " 'again': 590,\n",
       " 'asking': 591,\n",
       " 'list': 592,\n",
       " 'all': 593,\n",
       " 'command': 594,\n",
       " 'killed': 595,\n",
       " 'action': 596,\n",
       " 'Now': 597,\n",
       " 'owing': 598,\n",
       " 'allegiance': 599,\n",
       " 'none': 600,\n",
       " 'confronts': 601,\n",
       " 'destroys': 602,\n",
       " 'then': 603,\n",
       " 'goes': 604,\n",
       " 'separate': 605,\n",
       " 'ways': 606,\n",
       " 'begin': 607,\n",
       " 'lives': 608,\n",
       " 'Development': 609,\n",
       " 'Concept': 610,\n",
       " 'after': 611,\n",
       " 'finished': 612,\n",
       " 'full': 613,\n",
       " 'beginning': 614,\n",
       " 'shortly': 615,\n",
       " 'took': 616,\n",
       " 'approximately': 617,\n",
       " 'staff': 618,\n",
       " 'look': 619,\n",
       " 'popular': 620,\n",
       " 'response': 621,\n",
       " 'what': 622,\n",
       " 'wanted': 623,\n",
       " 'next': 624,\n",
       " 'Like': 625,\n",
       " 'predecessor': 626,\n",
       " 'wanting': 627,\n",
       " 'refine': 628,\n",
       " 'mechanics': 629,\n",
       " 'created': 630,\n",
       " 'come': 631,\n",
       " 'revolutionary': 632,\n",
       " 'idea': 633,\n",
       " 'warrant': 634,\n",
       " 'entry': 635,\n",
       " 'Speaking': 636,\n",
       " 'interview': 637,\n",
       " 'stated': 638,\n",
       " 'considered': 639,\n",
       " 'true': 640,\n",
       " 'sequel': 641,\n",
       " 'required': 642,\n",
       " 'amount': 643,\n",
       " 'trial': 644,\n",
       " 'error': 645,\n",
       " 'platform': 646,\n",
       " 'gave': 647,\n",
       " 'improve': 648,\n",
       " 'upon': 649,\n",
       " 'best': 650,\n",
       " 'parts': 651,\n",
       " 'In': 652,\n",
       " 'addition': 653,\n",
       " 'scenario': 654,\n",
       " 'written': 655,\n",
       " 'Hiroyuki': 656,\n",
       " 'Its': 657,\n",
       " 'darker': 658,\n",
       " 'somber': 659,\n",
       " 'majority': 660,\n",
       " 'material': 661,\n",
       " 'design': 662,\n",
       " 'improvements': 663,\n",
       " 'were': 664,\n",
       " 'made': 665,\n",
       " 'graphics': 666,\n",
       " 'layouts': 667,\n",
       " 'structure': 668,\n",
       " 'number': 669,\n",
       " 'playable': 670,\n",
       " 'upgrade': 671,\n",
       " 'involved': 672,\n",
       " 'models': 673,\n",
       " 'body': 674,\n",
       " 'achieve': 675,\n",
       " 'cooperative': 676,\n",
       " 'incorporated': 677,\n",
       " 'second': 678,\n",
       " 'removed': 679,\n",
       " 'memory': 680,\n",
       " 'space': 681,\n",
       " 'needed': 682,\n",
       " 'adjusted': 683,\n",
       " 'settings': 684,\n",
       " 'ease': 685,\n",
       " 'play': 686,\n",
       " 'could': 687,\n",
       " 'appeal': 688,\n",
       " 'retaining': 689,\n",
       " 'essential': 690,\n",
       " 'components': 691,\n",
       " 'newer': 692,\n",
       " 'systems': 693,\n",
       " 'decided': 694,\n",
       " 'designs': 695,\n",
       " 'worked': 696,\n",
       " 'When': 697,\n",
       " 'faced': 698,\n",
       " 'problem': 699,\n",
       " 'uniforms': 700,\n",
       " 'essentially': 701,\n",
       " 'individuality': 702,\n",
       " 'despite': 703,\n",
       " 'him': 704,\n",
       " 'needing': 705,\n",
       " 'create': 706,\n",
       " 'identify': 707,\n",
       " 'maintaining': 708,\n",
       " 'sense': 709,\n",
       " 'reality': 710,\n",
       " 'world': 711,\n",
       " 'color': 712,\n",
       " 'engine': 713,\n",
       " 'anime': 714,\n",
       " 'produced': 715,\n",
       " 'Production': 716,\n",
       " 'I.G.': 717,\n",
       " 'Music': 718,\n",
       " 'music': 719,\n",
       " 'originally': 720,\n",
       " 'heard': 721,\n",
       " 'about': 722,\n",
       " 'project': 723,\n",
       " 'thought': 724,\n",
       " 'light': 725,\n",
       " 'tone': 726,\n",
       " 'similar': 727,\n",
       " 'themes': 728,\n",
       " 'much': 729,\n",
       " 'expected': 730,\n",
       " 'An': 731,\n",
       " 'designed': 732,\n",
       " 'vision': 733,\n",
       " 'rejected': 734,\n",
       " 'He': 735,\n",
       " 'seven': 736,\n",
       " 'production': 737,\n",
       " 'need': 738,\n",
       " 'initially': 739,\n",
       " 'recorded': 740,\n",
       " 'orchestra': 741,\n",
       " 'guitar': 742,\n",
       " 'bass': 743,\n",
       " 'synthesizer': 744,\n",
       " 'before': 745,\n",
       " 'segments': 746,\n",
       " 'piece': 747,\n",
       " 'incorporating': 748,\n",
       " 'hopeful': 749,\n",
       " 'tune': 750,\n",
       " 'played': 751,\n",
       " 'ending': 752,\n",
       " 'modern': 753,\n",
       " 'divorced': 754,\n",
       " 'fantasy': 755,\n",
       " 'musical': 756,\n",
       " 'instruments': 757,\n",
       " 'constructed': 758,\n",
       " 'working': 759,\n",
       " 'synthesized': 760,\n",
       " 'felt': 761,\n",
       " 'incorporate': 762,\n",
       " 'live': 763,\n",
       " 'arranged': 764,\n",
       " 'several': 765,\n",
       " 'later': 766,\n",
       " 'tracks': 767,\n",
       " 'song': 768,\n",
       " 'If': 769,\n",
       " 'You': 770,\n",
       " 'Wish': 771,\n",
       " '...': 772,\n",
       " 'Kimi': 773,\n",
       " 'singer': 774,\n",
       " 'reason': 775,\n",
       " 'fought': 776,\n",
       " 'particular': 777,\n",
       " 'wish': 778,\n",
       " 'precious': 779,\n",
       " 'responsibility': 780,\n",
       " 'duty': 781,\n",
       " 'lyrics': 782,\n",
       " 'singles': 783,\n",
       " 'Release': 784,\n",
       " 'September': 785,\n",
       " 'teaser': 786,\n",
       " 'website': 787,\n",
       " 'revealed': 788,\n",
       " 'hinting': 789,\n",
       " 'issue': 790,\n",
       " 'Famitsu': 791,\n",
       " 'listed': 792,\n",
       " 'arriving': 793,\n",
       " 'public': 794,\n",
       " 'appearance': 795,\n",
       " 'Tokyo': 796,\n",
       " 'Game': 797,\n",
       " 'Show': 798,\n",
       " 'TGS': 799,\n",
       " 'demo': 800,\n",
       " 'available': 801,\n",
       " 'journalists': 802,\n",
       " 'attendees': 803,\n",
       " 'publicity': 804,\n",
       " 'details': 805,\n",
       " 'too': 806,\n",
       " 'potential': 807,\n",
       " 'still': 808,\n",
       " 'flux': 809,\n",
       " 'reveal': 810,\n",
       " 'promote': 811,\n",
       " 'detail': 812,\n",
       " 'leading': 813,\n",
       " 'episodic': 814,\n",
       " 'Flash': 815,\n",
       " 'visual': 816,\n",
       " 'novel': 817,\n",
       " '27': 818,\n",
       " 'said': 819,\n",
       " 'capacity': 820,\n",
       " 'DLC': 821,\n",
       " 'plans': 822,\n",
       " 'finalized': 823,\n",
       " 'Multiple': 824,\n",
       " 'featuring': 825,\n",
       " 'between': 826,\n",
       " 'February': 827,\n",
       " 'April': 828,\n",
       " 'Extra': 829,\n",
       " 'Edition': 830,\n",
       " '23': 831,\n",
       " 'sold': 832,\n",
       " 'lower': 833,\n",
       " 'price': 834,\n",
       " 'chosen': 835,\n",
       " 'pre': 836,\n",
       " 'bonus': 837,\n",
       " 'People': 838,\n",
       " 'owned': 839,\n",
       " 'transfer': 840,\n",
       " 'save': 841,\n",
       " 'data': 842,\n",
       " 'versions': 843,\n",
       " 'Unlike': 844,\n",
       " 'west': 845,\n",
       " 'According': 846,\n",
       " 'poor': 847,\n",
       " 'general': 848,\n",
       " 'unpopularity': 849,\n",
       " 'PSP': 850,\n",
       " 'unofficial': 851,\n",
       " 'patch': 852,\n",
       " '2012': 853,\n",
       " 'copy': 854,\n",
       " 'download': 855,\n",
       " 'apply': 856,\n",
       " 'translated': 857,\n",
       " 'English': 858,\n",
       " 'Reception': 859,\n",
       " 'On': 860,\n",
       " 'day': 861,\n",
       " 'topped': 862,\n",
       " 'exclusive': 863,\n",
       " 'multi': 864,\n",
       " 'charts': 865,\n",
       " 'By': 866,\n",
       " '102': 867,\n",
       " '@,@': 868,\n",
       " 'coming': 869,\n",
       " 'overall': 870,\n",
       " 'Last': 871,\n",
       " 'Story': 872,\n",
       " 'Wii': 873,\n",
       " '152': 874,\n",
       " '500': 875,\n",
       " 'enjoyed': 876,\n",
       " 'particularly': 877,\n",
       " 'pleased': 878,\n",
       " 'gaming': 879,\n",
       " 'site': 880,\n",
       " 'Watch': 881,\n",
       " 'negatively': 882,\n",
       " 'noting': 883,\n",
       " 'pacing': 884,\n",
       " 'recycled': 885,\n",
       " 'generally': 886,\n",
       " 'entertaining': 887,\n",
       " 'putting': 888,\n",
       " 'spikes': 889,\n",
       " 'writer': 890,\n",
       " 'Play': 891,\n",
       " 'Test': 892,\n",
       " 'article': 893,\n",
       " 'provided': 894,\n",
       " 'profound': 895,\n",
       " 'feeling': 896,\n",
       " 'closure': 897,\n",
       " 'annoying': 898,\n",
       " 'limitations': 899,\n",
       " 'aspects': 900,\n",
       " 'special': 901,\n",
       " 'abilities': 902,\n",
       " 'positively': 903,\n",
       " 'noted': 904,\n",
       " 'Official': 905,\n",
       " 'Magazine': 906,\n",
       " '-': 907,\n",
       " 'UK': 908,\n",
       " 'moral': 909,\n",
       " 'standing': 910,\n",
       " 'art': 911,\n",
       " 'style': 912,\n",
       " 'latter': 913,\n",
       " 'continued': 914,\n",
       " 'quality': 915,\n",
       " 'tweaks': 916,\n",
       " 'balance': 917,\n",
       " 'criticism': 918,\n",
       " 'affected': 919,\n",
       " 'Heath': 920,\n",
       " 'Hindman': 921,\n",
       " 'non': 922,\n",
       " 'removal': 923,\n",
       " 'praising': 924,\n",
       " 'returning': 925,\n",
       " 'serious': 926,\n",
       " 'Points': 927,\n",
       " 'criticized': 928,\n",
       " 'review': 929,\n",
       " 'awkward': 930,\n",
       " 'cutscenes': 931,\n",
       " 'seemed': 932,\n",
       " 'include': 933,\n",
       " 'scene': 934,\n",
       " 'good': 935,\n",
       " 'issues': 936,\n",
       " 'occasional': 937,\n",
       " 'problems': 938,\n",
       " 'AI': 939,\n",
       " 'preview': 940,\n",
       " 'Ryan': 941,\n",
       " 'Geddes': 942,\n",
       " 'IGN': 943,\n",
       " 'left': 944,\n",
       " 'excited': 945,\n",
       " 'go': 946,\n",
       " 'completing': 947,\n",
       " 'enjoying': 948,\n",
       " 'improved': 949,\n",
       " 'visuals': 950,\n",
       " 'Kotaku': 951,\n",
       " 'Richard': 952,\n",
       " 'highly': 953,\n",
       " 'citing': 954,\n",
       " 'form': 955,\n",
       " 'His': 956,\n",
       " 'criticisms': 957,\n",
       " 'length': 958,\n",
       " 'repetition': 959,\n",
       " 'expressing': 960,\n",
       " 'regret': 961,\n",
       " 'Legacy': 962,\n",
       " 'featured': 963,\n",
       " 'Nintendo': 964,\n",
       " '3DS': 965,\n",
       " 'crossover': 966,\n",
       " 'Project': 967,\n",
       " 'X': 968,\n",
       " 'Zone': 969,\n",
       " 'representing': 970,\n",
       " 'develop': 971,\n",
       " 'forms': 972,\n",
       " 'Adaptations': 973,\n",
       " 'episode': 974,\n",
       " 'Taken': 975,\n",
       " 'Sake': 976,\n",
       " 'Network': 977,\n",
       " 'planned': 978,\n",
       " 'availability': 979,\n",
       " 'period': 980,\n",
       " 'extended': 981,\n",
       " 'stoppage': 982,\n",
       " 'summer': 983,\n",
       " 'DVD': 984,\n",
       " 'June': 985,\n",
       " '29': 986,\n",
       " 'August': 987,\n",
       " '31': 988,\n",
       " 'Black': 989,\n",
       " 'Blue': 990,\n",
       " 'editions': 991,\n",
       " 'purchase': 992,\n",
       " 'set': 993,\n",
       " 'half': 994,\n",
       " 'detailing': 995,\n",
       " 'rivals': 996,\n",
       " 'announced': 997,\n",
       " '1': 998,\n",
       " 'Pictures': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_corpus.word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = original_corpus.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"half\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'will god let her fall or give her a new path \\\\? <eos>'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0] + ' <eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns the index of the word or the index of \"UNK\" otherwise\n",
    "word2idx.get(\"will\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 300, 5010, 7291,  361, 6356,  310, 2193,  361,   27,  578, 9428,\n",
       "          0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([word2idx.get(w, 0) for w in X_train[0].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(s, word2idx=word2idx, N=35):\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([word2idx.get(w, 0) for w in s.split()])\n",
    "    l = min(N, len(enc1))\n",
    "    enc[:l] = enc1[:l]\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 300, 5010, 7291,  361, 6356,  310, 2193,  361,   27,  578, 9428,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0], dtype=int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 35)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.vstack([encode_sentence(x) for x in X_train])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 35)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.vstack([encode_sentence(x) for x in X_test])\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Transfer learning from language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I am not bodering with mini-batches since our dataset is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train(model, x_train, y_train):\n",
    "model = SentenceCNN(V, D, glove_weights=pretrained_weight).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 0.698 and accuracy 0.490\n"
     ]
    }
   ],
   "source": [
    "test_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this filters parameters with p.requires_grad=True\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        x = Variable(torch.LongTensor(x_train)).cuda()\n",
    "        y = Variable(torch.Tensor(y_train)).cuda().unsqueeze(1)\n",
    "        y_hat = model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.data[0])\n",
    "    test_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metrics(m):\n",
    "    model.eval()\n",
    "    x = Variable(torch.LongTensor(x_test)).cuda()\n",
    "    y = Variable(torch.Tensor(y_test)).cuda().unsqueeze(1)\n",
    "    y_hat = m(x)\n",
    "    loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "    y_pred = y_hat > 0\n",
    "    correct = (y_pred.float() == y).float().sum()\n",
    "    accuracy = correct/pred.shape[0]\n",
    "    print(\"test loss %.3f and accuracy %.3f\" % (loss.data[0], accuracy.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7035446763038635\n",
      "1.1868393421173096\n",
      "0.9086019396781921\n",
      "0.5865945816040039\n",
      "0.3559541702270508\n",
      "0.3733510375022888\n",
      "0.4246615469455719\n",
      "0.4422186017036438\n",
      "0.4157627522945404\n",
      "0.3762800097465515\n",
      "test loss 0.360 and accuracy 0.860\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34370580315589905\n",
      "0.5294861793518066\n",
      "0.32328617572784424\n",
      "0.40581214427948\n",
      "0.2806266248226166\n",
      "0.2568565309047699\n",
      "0.30520763993263245\n",
      "0.29174017906188965\n",
      "0.24017520248889923\n",
      "0.22176489233970642\n",
      "test loss 0.285 and accuracy 0.892\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23691332340240479\n",
      "0.2138260155916214\n",
      "0.20660893619060516\n",
      "0.20420822501182556\n",
      "0.20270921289920807\n",
      "0.20497538149356842\n",
      "0.19893155992031097\n",
      "0.1915552318096161\n",
      "0.19009748101234436\n",
      "0.18447645008563995\n",
      "test loss 0.246 and accuracy 0.898\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([100, 300, 3]), torch.Size([100]), torch.Size([100, 300, 4]), torch.Size([100]), torch.Size([100, 300, 5]), torch.Size([100]), torch.Size([1, 300]), torch.Size([1])]\n"
     ]
    }
   ],
   "source": [
    "# how to figure out the parameters\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print([p.size() for p in parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreezing the embeddings\n",
    "model.embedding.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([400007, 300]), torch.Size([100, 300, 3]), torch.Size([100]), torch.Size([100, 300, 4]), torch.Size([100]), torch.Size([100, 300, 5]), torch.Size([100]), torch.Size([1, 300]), torch.Size([1])]\n"
     ]
    }
   ],
   "source": [
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print([p.size() for p in parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1830516755580902\n",
      "0.17343704402446747\n",
      "0.16393160820007324\n",
      "0.15581797063350677\n",
      "0.14433661103248596\n",
      "0.1368999481201172\n",
      "0.1289738416671753\n",
      "0.12384654581546783\n",
      "0.11491061747074127\n",
      "0.1075991541147232\n",
      "test loss 0.228 and accuracy 0.913\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10160469263792038\n",
      "0.10249238461256027\n",
      "0.10043996572494507\n",
      "0.10032773017883301\n",
      "0.09892096370458603\n",
      "0.09907413274049759\n",
      "0.09923809766769409\n",
      "0.09644830971956253\n",
      "0.09582842141389847\n",
      "0.0971798449754715\n",
      "test loss 0.227 and accuracy 0.912\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show how to save model\n",
    "* Show how to predict on new data\n",
    "* Test a version with a smaller word embedding matrix\n",
    "* Try Another tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You may not need to keep all word embeddings.\n",
    "* Extend this code by finetunning the embedding layer.\n",
    "* Use fasttext instead of globe model. (https://fasttext.cc/docs/en/english-vectors.html)\n",
    "\n",
    "   `! pip install git+https://github.com/facebookresearch/fastText.git`\n",
    "* Extend this code to do cross-validation. Look at https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py for an example on how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Language model from here https://github.com/pytorch/examples/blob/master/word_language_model/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
