{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sequence to Sequence with Attention\n",
    "In this project we will be teaching a neural network to translate from\n",
    "French to English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is made possible by the simple but powerful idea of the [sequence\n",
    "to sequence network](https://arxiv.org/abs/1409.3215>), in which two\n",
    "recurrent neural networks work together to transform one sequence to\n",
    "another. An encoder network condenses an input sequence into a vector,\n",
    "and a decoder network unfolds that vector into a new sequence.\n",
    "\n",
    "To improve upon this model we'll use an [attention\n",
    "mechanism](https://arxiv.org/abs/1409.0473), which lets the decoder\n",
    "learn to focus over a specific range of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "The data for this project is a set of many thousands of English to\n",
    "French translation pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset():\n",
    "    ! wget https://download.pytorch.org/tutorial/data.zip\n",
    "    ! unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://www.manythings.org/anki/spa-eng.zip\n",
    "# mv _about.txt spa.txt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download the dataset\n",
    "#download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a unique index per word to use as the inputs and targets of\n",
    "the networks later. To keep track of all this we will use a helper class\n",
    "called ``Lang`` which has word → index (``word2index``) and index → word\n",
    "(``index2word``) dictionaries, as well as a count of each word\n",
    "``word2count`` to use to later replace rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"PAD\": 0, \"SOS\": 1, \"EOS\": 2, \"UNK\": 3}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"UNK\"}\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are all in Unicode, to simplify we will turn Unicode\n",
    "characters to ASCII, make everything lowercase, and trim most\n",
    "punctuation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    \"\"\"Turn a Unicode string to plain ASCII\n",
    "    \n",
    "    https://stackoverflow.com/a/518232/2809427\n",
    "    \"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    \"\"\"Lowercase, trim, and remove non-letter characters\"\"\"\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(filename):\n",
    "    # Read the file and split into lines\n",
    "    lines = open(filename).read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15\n",
    "\n",
    "# Selecting some sentences\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "# Filtering by sentence length\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[0].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(data_filename):\n",
    "    pairs = readLangs(data_filename)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    \n",
    "    #randomize the data with a fixed seed for repeatability\n",
    "    random.seed(4)\n",
    "    random.shuffle(pairs)\n",
    "    #choose the first 10 pairs for testing and the rest for training\n",
    "    valid_pairs = pairs[0:200]\n",
    "    train_pairs = pairs[200:len(pairs)]\n",
    "    \n",
    "    print(\"number of valid pairs: %s\" % len(valid_pairs))\n",
    "    print(\"number of train pairs: %s\" % len(train_pairs))\n",
    "    \n",
    "    input_lang = Lang(\"english\")\n",
    "    output_lang = Lang(\"french\")\n",
    "    \n",
    "    print(\"Counting words...\")\n",
    "    cnt = 0\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[1])\n",
    "        output_lang.addSentence(pair[0])\n",
    "        \n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs, train_pairs, valid_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 135842 sentence pairs\n",
      "Trimmed to 12823 sentence pairs\n",
      "number of valid pairs: 200\n",
      "number of train pairs: 12623\n",
      "Counting words...\n",
      "Counted words:\n",
      "english 5036\n",
      "french 3310\n",
      "['we re still vulnerable .', 'nous sommes encore vulnerables .']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs, train_pairs, valid_pairs = prepareData(\"data/eng-fra.txt\")\n",
    "random.seed(4)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you re charming .', 'tu es charmant .']"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(s, vocab2index, N=12, padding_start=True):\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([SOS_token] + [vocab2index.get(w, vocab2index[\"UNK\"]) for w in s.split()] + [EOS_token])\n",
    "    l = min(N, len(enc1))\n",
    "    if padding_start:\n",
    "        enc[:l] = enc1[:l]\n",
    "    else:\n",
    "        enc[N-l:] = enc1[:l]\n",
    "    return enc, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you re charming .', 'tu es charmant .']"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  0,  0,  0,  0,  1,  3,  3,  3, 11,  2], dtype=int32), 6)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence(train_pairs[0][0], input_lang.word2index, padding_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  3,  3,  3, 10,  2,  0,  0,  0,  0,  0,  0], dtype=int32), 6)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence(train_pairs[0][1], output_lang.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, pairs, input_lang, output_lang):\n",
    "        self.pairs = pairs\n",
    "        self.input_word2index = input_lang.word2index\n",
    "        self.output_word2index = output_lang.word2index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, n_x = encode_sentence(self.pairs[idx][1], self.input_word2index, padding_start=False)\n",
    "        y, n_y = encode_sentence(self.pairs[idx][0], self.output_word2index)\n",
    "        return x, y\n",
    "    \n",
    "train_ds = PairDataset(train_pairs, input_lang, output_lang)\n",
    "valid_ds = PairDataset(valid_pairs, input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0,   0,   0,   0,   1,  40,  41, 446,  11,   2],\n",
       "       dtype=int32),\n",
       " array([  1,  11,  12, 383,  10,   2,   0,   0,   0,   0,   0,   0],\n",
       "       dtype=int32))"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Seq2Seq Model\n",
    "\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <https://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "![](imgs/encoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.gru(x)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   0,    0,    0,    0,    0,    1,   12,   13,  179, 1977,   11,    2],\n",
       "         [   0,    0,    0,    0,    0,    0,    1,   12,   13, 1814,   11,    2],\n",
       "         [   0,    0,    0,    0,    0,    1,   56,   65,   26, 3706,   11,    2],\n",
       "         [   0,    0,    0,    1,   76,   52,   54,  114,   12, 2761,   11,    2],\n",
       "         [   1,  137, 1098, 1039,   19,  271,   68,    6,  238,   29, 2467,   11]],\n",
       "        dtype=torch.int32),\n",
       " tensor([[   1,   11,   12,  147, 1243,   10,    2,    0,    0,    0,    0,    0],\n",
       "         [   1,   11,   12, 1392,   10,    2,    0,    0,    0,    0,    0,    0],\n",
       "         [   1,   38,   46,   22,  269,   10,    2,    0,    0,    0,    0,    0],\n",
       "         [   1,   56,   52,   14,   19, 2018,   11,   10,    2,    0,    0,    0],\n",
       "         [   1,  111,   57,  862,  232,   93,  204,  113, 1832,   10,    2,    0]],\n",
       "        dtype=torch.int32))"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = input_lang.n_words\n",
    "hidden_size = 2\n",
    "encoder = EncoderRNN(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_outputs, enc_hidden = encoder(x.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 12, 2]), torch.Size([1, 5, 2]))"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_outputs.shape, enc_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Attention Decoder\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention allows the decoder network to \"focus\" on a different part of\n",
    "the encoder's outputs for every step of the decoder's own outputs. First\n",
    "we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDotDecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDotDecoderRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, enc_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        output, hidden = self.gru(embedded, last_hidden)\n",
    "        \n",
    "        # Calculate attention weights from the current GRU hidden\n",
    "        attention_scores = torch.einsum('ijk,ik->ij', [enc_outputs, hidden[-1]])\n",
    "        attn_weights = F.softmax(attention_scores, dim=1)    \n",
    "        \n",
    "        # apply attention to enc_outputs\n",
    "        attn_applied = torch.einsum('ijk,ij->ik', [enc_outputs, attn_weights])\n",
    "        \n",
    "        concat_input = torch.cat((output.squeeze(1), attn_applied), 1) \n",
    "        concat_output = F.relu(self.attn_combine(concat_input))\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = output_lang.n_words\n",
    "hidden_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(output_size, hidden_size)\n",
    "attn = nn.Linear(hidden_size * 2, 1)\n",
    "attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
    "dropout = nn.Dropout(0.1)\n",
    "gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "out = nn.Linear(hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = y.size(0)\n",
    "decoder_input = SOS_token*torch.ones(batch_size,1)\n",
    "decoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = enc_hidden\n",
    "xx = decoder_input\n",
    "xx = embedding(xx.long())\n",
    "output, hidden = gru(xx, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 2])"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_length = 12\n",
    "attention_scores = torch.zeros(batch_size, enc_length)\n",
    "for ei in range(enc_length):\n",
    "    attention_scores[:, ei] = (enc_outputs[:, ei, :]*hidden[-1]).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12])"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.einsum('ijk,ik->ij', [enc_outputs, hidden[-1]])\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores==c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12])"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = F.softmax(attention_scores, dim=1)\n",
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12, 2])"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2*torch.ones(2,12,2)\n",
    "b = 3*torch.ones(2,12)\n",
    "c = torch.einsum('ijk,ij->ijk', [a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12, 2])"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_applied = torch.einsum('ijk,ij->ijk', [enc_outputs, attn_weights])\n",
    "attn_applied.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = torch.einsum('ijk,ik->ij', [enc_outputs, hidden[-1]])\n",
    "attn_weights = F.softmax(attention_scores, dim=1)    \n",
    "        \n",
    "attn_applied = torch.einsum('ijk,ij->ik', [enc_outputs, attn_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_applied.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_input = torch.cat((output.squeeze(1), attn_applied), 1) \n",
    "concat_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_output = F.relu(attn_combine(concat_input))\n",
    "output = out(concat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the whole thing\n",
    "batch_size = y.size(0)\n",
    "decoder_input = SOS_token*torch.ones(batch_size,1).long()\n",
    "decoder = AttnDotDecoderRNN(output_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hidden, attn_weights = decoder(decoder_input, hidden, enc_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but `when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation -\n",
    "intuitively it has learned to represent the output grammar and can \"pick\n",
    "up\" the meaning once the teacher tells it the first few words, but it\n",
    "has not properly learned how to create the sentence from the translation\n",
    "in the first place.\n",
    "\n",
    "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "``teacher_forcing_ratio`` up to use more of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(x, y, encoder, decoder, encoder_optimizer, decoder_optimizer, teacher_forcing_ratio=0.5):\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    batch_size = y.size(0)\n",
    "    target_length = y.size(1)\n",
    "\n",
    "    enc_outputs, enc_hidden = encoder(x)\n",
    "\n",
    "    loss = 0\n",
    "    dec_input = y[:,0].unsqueeze(1) # allways SOS\n",
    "    hidden = enc_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    for di in range(1, target_length):\n",
    "        output, hidden, attention = decoder(dec_input, hidden, enc_outputs)\n",
    "        yi =  y[:, di]\n",
    "        if (yi>0).sum() > 0:\n",
    "            loss += F.cross_entropy(output, yi, ignore_index = 0, reduction=\"sum\")/(yi>0).sum()\n",
    "        if use_teacher_forcing:\n",
    "            dec_input = y[:, di].unsqueeze(1)  # Teacher forcing: Feed the target as the next input\n",
    "        else:                \n",
    "            dec_input = output.argmax(dim=1).unsqueeze(1).detach()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, enc_optimizer, dec_optimizer, epochs = 10, teacher_forcing_ratio=0.5):\n",
    "    for i in range(epochs):\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        for x, y in train_dl:\n",
    "            x = x.long()\n",
    "            y = y.long()\n",
    "            loss = train_batch(x, y, encoder, decoder, enc_optimizer, dec_optimizer, teacher_forcing_ratio)\n",
    "            total_loss = loss*x.size(0)\n",
    "            total += x.size(0)\n",
    "        if i % 10 == 0:\n",
    "            print(\"train loss %.3f\" % (total_loss / total))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = input_lang.n_words\n",
    "output_size = output_lang.n_words\n",
    "hidden_size = 300\n",
    "encoder = EncoderRNN(input_size, hidden_size)\n",
    "decoder = AttnDotDecoderRNN(output_size, hidden_size)\n",
    "enc_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 500\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.376\n",
      "train loss 0.223\n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, enc_optimizer, dec_optimizer, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.101\n",
      "train loss 0.101\n",
      "train loss 0.084\n",
      "train loss 0.061\n"
     ]
    }
   ],
   "source": [
    "enc_optimizer = optim.Adam(encoder.parameters(), lr=0.0001)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(), lr=0.0001) \n",
    "train(encoder, decoder, enc_optimizer, dec_optimizer, epochs = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.067\n",
      "train loss 0.101\n",
      "train loss 0.088\n",
      "train loss 0.047\n",
      "train loss 0.061\n",
      "train loss 0.054\n",
      "train loss 0.052\n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, enc_optimizer, dec_optimizer, epochs = 300, teacher_forcing_ratio=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(encoder, decoder, enc_optimizer, dec_optimizer, epochs = 300, teacher_forcing_ratio=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))\n",
    "enc_outputs, enc_hidden = encoder(x.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_input.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "di = 0\n",
    "dec_input = y[:,0].unsqueeze(1).long() # it is always \n",
    "hidden = enc_hidden\n",
    "output, hidden, attention = decoder(dec_input, hidden, enc_outputs) \n",
    "loss += F.cross_entropy(output, y[:, di].long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_input = output.argmax(dim=1).unsqueeze(1).detach()\n",
    "dec_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = 1\n",
    "output, hidden, attention = decoder(dec_input, hidden, enc_outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss += F.cross_entropy(output, y[:, di].long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.argmax(dim=1).unsqueeze(1).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x, encoder, decoder, max_length=12):\n",
    "    decoder = decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        enc_outputs, enc_hidden = encoder(x)\n",
    "        dec_input = SOS_token*torch.ones(batch_size, 1).long()  # SOS\n",
    "        hidden = enc_hidden\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(batch_size, max_length, max_length)\n",
    "\n",
    "        for di in range(1, max_length):\n",
    "            output, hidden, attention = decoder(dec_input, hidden, enc_outputs)\n",
    "            decoder_attentions[:, di, :] = attention.detach()\n",
    "            pred = output.argmax(dim=1)\n",
    "            decoded_words.append(pred.numpy())\n",
    "            dec_input = output.argmax(dim=1).unsqueeze(1).detach()\n",
    "\n",
    "        return np.transpose(decoded_words), decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "train_dl_2 = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x, y = next(iter(train_dl_2)) \n",
    "x = x.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_words, decoder_attentions = evaluate(x, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_outputs, enc_hidden = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(x, y, encoder, decoder, attention=False):\n",
    "    decoded_words, attentions = evaluate(x, encoder, decoder)\n",
    "    for i in range(x.shape[0]):\n",
    "        xi = x[i].numpy()\n",
    "        yi = y[i].numpy()\n",
    "        y_hat = decoded_words[i]\n",
    "        x_sent = ' '.join([input_lang.index2word[t] for t in xi if t > 3])\n",
    "        y_sent = ' '.join([output_lang.index2word[t] for t in yi if t > 3])\n",
    "        y_hat_sent = ' '.join([output_lang.index2word[t] for t in y_hat if t > 3])\n",
    "        print('>', x_sent)\n",
    "        print('=', y_sent)\n",
    "        print('<', y_hat_sent)\n",
    "        print('')\n",
    "        if attention:\n",
    "            showAttention(input_sentence, output_words, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(x, y, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "valid_dl_2 = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x, y = next(iter(valid_dl_2)) \n",
    "x = x.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(x, y, encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Attention\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable\n",
    "outputs. Because it is used to weight specific encoder outputs of the\n",
    "input sequence, we can imagine looking where the network is focused most\n",
    "at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(x, y, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "=========\n",
    "-  Replace the embeddings with pre-trained word embeddings. Here are word embeddings for various languages.\n",
    "\n",
    "https://fasttext.cc/docs/en/crawl-vectors.html \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "The original notebook was written by Sean Robertson <https://github.com/spro/practical-pytorch>_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
