{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with feed forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: What is pytorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a popular framework for deep learning.\n",
    "\n",
    "PyTorch consists of 4 main packages:\n",
    "\n",
    "* torch: a general purpose array library similar to Numpy that can do computations on GPU when the tensor type is cast to (torch.cuda.TensorFloat)\n",
    "* torch.autograd: a package for building a computational graph and automatically obtaining gradients\n",
    "* torch.nn: a neural net library with common layers and cost functions\n",
    "* torch.optim: an optimization package with common optimization algorithms like SGD,Adam, etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "Here we load the dataset. In the future we will create our own datasets but MNIST dataset is part of Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = datasets.MNIST('../data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "test_ds = datasets.MNIST('../data', train=False, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset in Pytorch is a  subclass of ```torch.utils.data.Dataset```  thas has methods ```__getitem__``` and ```__len__``` methods implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# has length\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "   -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.0424\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.1995  2.6051\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1951  2.3633\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.5940\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1315\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1951  1.7523  2.3633\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242  0.2758  1.7650  2.4524  2.7960  2.7960\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242  1.3068  2.7960  2.7960  2.7960  2.2742\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       " \n",
       " Columns 9 to 17 \n",
       "   -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.3860 -0.1951 -0.1951 -0.1951  1.1795  1.3068\n",
       "   0.0340  0.7722  1.5359  1.7396  2.7960  2.7960  2.7960  2.7960  2.7960\n",
       "   2.7960  2.7960  2.7960  2.7960  2.7960  2.7960  2.7960  2.7960  2.7706\n",
       "   2.7960  2.7960  2.7960  2.7960  2.7960  2.0960  1.8923  2.7197  2.6433\n",
       "   1.5614  0.9377  2.7960  2.7960  2.1851 -0.2842 -0.4242  0.1231  1.5359\n",
       "  -0.2460 -0.4115  1.5359  2.7960  0.7213 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242  1.3450  2.7960  1.9942 -0.3988 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.2842  1.9942  2.7960  0.4668 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242  0.0213  2.6433  2.4396  1.6123  0.9504 -0.4115\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242  0.6068  2.6306  2.7960  2.7960  1.0904\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.1486  1.9432  2.7960  2.7960\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.2206  0.7595  2.7833\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  2.7451\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.1613  1.2305  1.9051  2.7960\n",
       "  -0.4242 -0.4242 -0.4242  0.0722  1.4596  2.4906  2.7960  2.7960  2.7960\n",
       "  -0.4242 -0.1187  1.0268  2.3887  2.7960  2.7960  2.7960  2.7960  2.1342\n",
       "   0.4159  2.2869  2.7960  2.7960  2.7960  2.7960  2.0960  0.6068 -0.3988\n",
       "   2.7960  2.7960  2.7960  2.7960  2.0578  0.5940 -0.3097 -0.4242 -0.4242\n",
       "   2.7960  2.7960  2.6815  1.2686 -0.2842 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "   1.2941  1.2559 -0.2206 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       " \n",
       " Columns 18 to 26 \n",
       "   -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "   1.8032 -0.0933  1.6887  2.8215  2.7197  1.1923 -0.4242 -0.4242 -0.4242\n",
       "   2.4396  1.7650  2.7960  2.6560  2.0578  0.3904 -0.4242 -0.4242 -0.4242\n",
       "   0.7595  0.6195  0.6195  0.2886  0.0722 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.1060 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "   1.4850 -0.0806 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "   2.7960  1.9560 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "   2.7960  2.7451  0.3904 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "   2.7960  2.2105 -0.3988 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "   2.7578  1.8923 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "   0.5686 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
       " \n",
       " Columns 27 to 27 \n",
       "   -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       "  -0.4242\n",
       " [torch.FloatTensor of size 1x28x28], 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can index any element\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# train_ds[0] is a tuple with an image (x) and a class (y)\n",
    "x, y = train_ds[0]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "Data loader combines a dataset and a sampler, and provides an iterator over the dataset. The data loader divides the data in mini batches. This is particularly important when working with large dataset that cannot be hold in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False) # for test we use shuffle=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = iter(train_loader)\n",
    "x, y = next(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img, title=None):\n",
    "    plt.imshow(img, interpolation='none', cmap=\"gray\")\n",
    "    if title is not None: plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1, 28, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first from torch to numpy\n",
    "X = x.numpy(); Y = y.numpy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(X[0][0], Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn module\n",
    "a neural net library with common layers and cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear transformation of a Nx5 matrix into a Nx3 matrix, where N can be anything\n",
    "D = 5 # number of input featutes\n",
    "M = 3 # neurons in the first hidden layer\n",
    "linear_map = nn.Linear(D, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       "  0.3818 -0.4302 -0.4161  0.3176  0.0683\n",
       "  0.0464  0.0697 -0.2805  0.1885 -0.0439\n",
       " -0.2053  0.0112  0.2437  0.1981  0.2265\n",
       " [torch.FloatTensor of size 3x5], Parameter containing:\n",
       "  0.1039\n",
       "  0.3674\n",
       "  0.4132\n",
       " [torch.FloatTensor of size 3]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters are initialized randomly\n",
    "[p for p in linear_map.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.0712 -0.3075 -0.0283 -0.6889  0.1833\n",
       " 1.5569  0.3716 -0.0614  1.7085 -1.7961\n",
       "-0.1991 -0.1137 -0.8812  1.0450 -0.8976\n",
       " 0.7874 -0.9994  1.9837  1.5629 -0.0603\n",
       " 0.6944 -0.8675 -1.2912 -1.0709 -1.1637\n",
       "-0.7981 -0.2982  0.8684  0.3303 -0.1808\n",
       " 2.4560  1.0767 -1.9960  1.1347  1.3043\n",
       "-1.4543 -1.0590  0.4774 -1.6150  1.5099\n",
       "-1.5020  0.5714  0.9445 -0.7971  1.4647\n",
       " 0.1477 -1.4358  1.2585  0.2768  0.0726\n",
       "[torch.FloatTensor of size 10x5]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random tensor\n",
    "# think about A as a dataset with 10 points and 5 features\n",
    "# the linear map with transform \n",
    "N = 10 \n",
    "A = Variable(torch.randn(N, D)) # In future versions of pytorch `Variable` would not be necessary\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.7492  0.1200  0.7330\n",
       " 0.9839  0.8837  0.0146\n",
       " 0.7140  0.8339  0.2418\n",
       " 0.5012  0.0752  1.0198\n",
       " 0.8599  0.5506 -0.5295\n",
       "-0.3414  0.1363  0.8098\n",
       " 1.8586  1.2729 -0.0452\n",
       "-0.6043 -0.2785  0.8381\n",
       "-1.2617 -0.1420  1.1318\n",
       " 0.3470 -0.0298  0.7448\n",
       "[torch.FloatTensor of size 10x3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_map(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating models \n",
    "There are two ways of define a model in Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "To create a model with nn.Sequential you provide a list of layers. For example, the following model defines a 2-layer neural network with 784 input features ($D = 784$), 300 hidden layers ($M=300$) and 10 outputs. This model uses Relu activation funtion and no final activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(784, 300),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(300, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module\n",
    "A more flexible way to define models in pytorch is as a subclass of nn.Module. At the ```__init__``` method we define all layers that will be used later. In the forward method, we should propose steps how we want to use already defined layers. Here is the same example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear1 = nn.Linear(784, 300)\n",
    "        self.linear2 = nn.Linear(300, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://hsaghir.github.io/data_science/pytorch_starter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
